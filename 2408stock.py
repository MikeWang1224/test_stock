#8110stock 


# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py  (8110stock.py)
- Attention-LSTM
- Multi-task: Return path + Direction
- âœ… å°è³‡æ–™å‹å–„ç‰ˆï¼šæ›´ç©©ã€æ›´ä¸å®¹æ˜“äº‚å™´
  1) LOOKBACK=40, STEPS=5
  2) LSTM + Attention poolingï¼ˆåƒæ•¸æ¯” Transformer æ›´é©åˆå°è³‡æ–™ï¼‰
  3) âœ… Return head åŠ  tanh é™å¹…ï¼ˆé¿å…é æ¸¬çˆ†ç‚¸ï¼‰
  4) âœ… Volume åš log1pï¼ˆå°è³‡æ–™æ›´ç©©ï¼‰ 
- åœ–è¡¨è¼¸å‡ºå®Œå…¨ä¸è®Šï¼ˆä¿ç•™ Today æ¨™è¨˜ï¼‰

âœ… æ”¹1ï¼šä¿®æ­£ scaler fit / split åº§æ¨™ç³»ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼ˆleakageï¼‰
  - create_sequences å›å‚³æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æ—¥æœŸ idx
  - split ç”¨æ¨£æœ¬æ•¸åˆ‡ï¼Œscaler.fit åªç”¨ train å€é–“çš„ df ç‰¹å¾µ

âœ… æ–°å¢ï¼šåŒæ™‚è¼¸å‡º PNG + CSVï¼ˆæª”åå« tickerï¼‰
  - results/YYYY-MM-DD_TICKER_pred.png
  - results/YYYY-MM-DD_TICKER_forecast.csv
  - results/YYYY-MM-DD_TICKER_backtest.png
  - results/YYYY-MM-DD_TICKER_backtest.csv

âœ… è¯æ± 8110.TW å°ˆå±¬å¼·åŒ–ï¼ˆç…§å‰é¢å»ºè­°æ”¹ï¼‰
  A) Featureï¼šåŠ å…¥ HL_RANGE / GAP / VOL_RELï¼ˆæ›´è²¼è¿‘ä¸­å°å‹è‚¡/æ³¢å‹•è‚¡ï¼‰
  B) Targetï¼šé æ¸¬ã€Œæ³¢å‹•æ¨™æº–åŒ–ã€log-returnï¼ˆç”¨ t-1 çš„ RET_STD_20 åšå°ºåº¦ï¼Œé¿å…å·çœ‹ï¼‰
  C) æ¨å›åƒ¹æ ¼æ™‚ï¼šæŠŠé æ¸¬çš„ normalized return ä¹˜å› asof çš„ RET_STD_20
  D) loss_weightsï¼šdirection æ¬Šé‡æé«˜ï¼ˆæ–¹å‘é€šå¸¸æ¯”ç²¾æº–åƒ¹æ›´å¯é ï¼‰
"""

import os, json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout,
    Softmax, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()

# ================= Firestore è®€å– =================
def load_df_from_firestore(
    ticker,
    collection="NEW_stock_data_liteon",
    days=500
):
    if db is None:
        raise ValueError("âŒ Firestore æœªåˆå§‹åŒ–")

    rows = []

    for doc in db.collection(collection).stream():
        p = doc.to_dict().get(ticker)
        if p:
            rows.append({
                "date": doc.id,   # YYYY-MM-DD
                **p
            })

    if not rows:
        raise ValueError("âš ï¸ Firestore ç„¡è³‡æ–™")

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["date"])

    # âœ… é€™è£¡æ‰æ˜¯ã€Œé˜²å‡æ—¥çš„ç¬¬ä¸€é“é–€ã€
    df = (
        df.sort_values("date")
          .tail(days)          # åªä¿ç•™æœ€è¿‘ N ç­†ã€Œäº¤æ˜“æ—¥ã€
          .set_index("date")
    )
    return df



# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_latest_trading_row(df):
    """
    è‹¥ä»Šå¤©æ˜¯éäº¤æ˜“æ—¥ï¼Œè£œ rowï¼ˆforward fillï¼‰
    ä½† Close ä¸æœƒè®Šï¼Œç”¨æ–¼ã€Œé æ¸¬ today+1ã€
    """
    today = pd.Timestamp(datetime.now().date())
    last = df.index.max()

    if last.normalize() >= today:
        return df

    all_days = pd.bdate_range(last, today)

    for d in all_days[1:]:
        if d not in df.index:
            df.loc[d] = df.loc[last]

    return df.sort_index()


def get_asof_trading_day(df: pd.DataFrame):
    """
    å›å‚³ (asof_date, is_today_trading)
    - è‹¥ä»Šå¤©æ˜¯äº¤æ˜“æ—¥ â†’ ç”¨ä»Šå¤©
    - è‹¥ä»Šå¤©éäº¤æ˜“æ—¥ â†’ ç”¨æœ€è¿‘ä¸€å€‹äº¤æ˜“æ—¥
    """
    today = pd.Timestamp(datetime.now().date())
    last_trading_day = df.index.max()

    if last_trading_day.normalize() == today:
        return last_trading_day, True
    else:
        return last_trading_day, False



# ================= Feature Engineering =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # ===== Volume ç©©å®šåŒ– =====
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    close = df["Close"].astype(float)

    # ===== log return =====
    logret = np.log(close).diff()

    # ===== RET_STD_20ï¼ˆçµ¦ normalized return ç”¨ï¼‰=====
    df["RET_STD_20"] = logret.rolling(20).std()

    # ===== åœ–è¡¨ç”¨å‡ç·šï¼ˆä¸å½±éŸ¿æ¨¡å‹ï¼‰=====
    df["SMA5"] = close.rolling(5).mean()
    df["SMA10"] = close.rolling(10).mean()

    return df


# ================= Sequenceï¼ˆé¿å…éŒ¯ä½ï¼Œä¸”ä¸äº‚åˆ‡ dfï¼‰ =================
def create_sequences(df, features, steps=5, window=40):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 çš„ log return
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆsum future_ret > 0ï¼‰
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰çš„ã€Œt ç•¶å¤©æ—¥æœŸã€
    """
    X, y_ret, y_dir, idx = [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()
    ret_std = df["RET_STD_20"].astype(float).values
    feat = df[features].values
    
    for i in range(window, len(df) - steps):
        x_seq = feat[i - window:i]
    
        scale = ret_std[i - 1]   # ç”¨ t-1 çš„æ³¢å‹•
        if not np.isfinite(scale) or scale <= 0:
            continue
    
        future_ret = logret.iloc[i:i + steps].values / scale
    
        if np.any(np.isnan(future_ret)) or np.any(np.isnan(x_seq)):
            continue
    
        X.append(x_seq)
        y_ret.append(future_ret)
        y_dir.append(1.0 if future_ret.sum() > 0 else 0.0)
        idx.append(df.index[i])


    return np.array(X), np.array(y_ret), np.array(y_dir), np.array(idx)

# ================= Lossï¼ˆdirection ç”¨ focalï¼›ä¸æ”¯æ´å°± fallbackï¼‰ =================
def get_direction_loss():
    if hasattr(tf.keras.losses, "BinaryFocalCrossentropy"):
        return tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0)

    def weighted_bce(y_true, y_pred, pos_weight=1.5):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-7, 1.0 - 1e-7)
        bce = -(y_true * tf.math.log(y_pred) + (1.0 - y_true) * tf.math.log(1.0 - y_pred))
        w = y_true * pos_weight + (1.0 - y_true) * 1.0
        return tf.reduce_mean(w * bce)

    return weighted_bce

# ================= Model buildï¼ˆreturn é™å¹… + æ–¹å‘èˆ‡returnå°é½Šï¼‰ =================
def build_attention_lstm(input_shape, steps, max_daily_logret=0.06, dir_from_ret_weight=2.0):
    inp = Input(shape=input_shape)

    x = LSTM(64, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    raw = Dense(steps, activation="tanh", name="raw_returns")(context)
    out_ret = Lambda(lambda t: t * max_daily_logret, name="return")(raw)

    base_logit = Dense(1, activation=None, name="dir_base_logit")(context)
    sum_raw = Lambda(lambda r: tf.reduce_sum(r, axis=1, keepdims=True), name="sum_raw")(raw)
    dir_logit = Lambda(lambda t: t[0] + dir_from_ret_weight * t[1], name="dir_logit")([base_logit, sum_raw])
    out_dir = Lambda(lambda z: tf.sigmoid(z), name="direction")(dir_logit)

    model = Model(inp, [out_ret, out_dir])
    return model

def compile_model(model, direction_weight=0.8, lr=7e-4):
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": get_direction_loss()
        },
        loss_weights={
            "return": 1.0,
            "direction": float(direction_weight)
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆToday æ¨™è¨˜ï¼Œæª”ååŠ  tickerï¼‰ =================
def plot_and_save(df_hist, future_df, ticker):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18, 8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}", fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}", color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title(f"{ticker} Attention-LSTM é æ¸¬")

    os.makedirs("results", exist_ok=True)
    plt.savefig(f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png", dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
def plot_backtest_error(df, ticker: str):
    """
    æ±ºç­–å¼å›æ¸¬åœ–ï¼ˆDecision-based Backtestï¼‰

    åš´æ ¼å®šç¾©ï¼š
    - å›æ¸¬ä¸€å®šä½¿ç”¨ã€Œæ˜¨å¤©æˆ–æ›´æ—©ã€ç”¢ç”Ÿçš„ forecast
    - t / t+1 ç‚ºæœ€å¾Œå…©å€‹ã€å·²å®Œæˆçš„çœŸå¯¦äº¤æ˜“æ—¥ã€
    - çµ•ä¸ä½¿ç”¨ä»Šå¤© forecastï¼ˆé¿å…å·çœ‹æœªä¾†ï¼‰
    """

    # === 1ï¸âƒ£ åªä¿ç•™ã€ŒçœŸå¯¦äº¤æ˜“æ—¥ã€ï¼ˆæ’é™¤ ensure_latest_trading_row è£œçš„å‡æ—¥ï¼‰===
    real_df = df.copy()
    real_df = real_df[real_df["Close"].diff().abs() > 1e-9]

    if len(real_df) < 3:
        print("âš ï¸ çœŸå¯¦äº¤æ˜“æ—¥ä¸è¶³ï¼Œç•¥éå›æ¸¬")
        return

    # === 2ï¸âƒ£ å®šç¾© t / t+1ï¼ˆæœ€å¾Œå…©å€‹å®Œæˆäº¤æ˜“æ—¥ï¼‰===
    valid_days = real_df.index
    t  = valid_days[-2]   # decision dayï¼ˆæ˜¨å¤©ï¼‰
    t1 = valid_days[-1]   # actual dayï¼ˆä»Šå¤©å·²æ”¶ç›¤ï¼‰

    # === 3ï¸âƒ£ å¾ results æ‰¾ã€Œâ‰¤ t çš„æœ€è¿‘ä¸€ç­† forecastã€===
    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    forecast_files = []
    for f in os.listdir("results"):
        if not f.endswith(f"_{ticker}_forecast.csv"):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
        except Exception:
            continue

        # â­ æ ¸å¿ƒæ¢ä»¶ï¼šforecast æ—¥æœŸ â‰¤ æ±ºç­–æ—¥ t
        if d <= t:
            forecast_files.append((d, f))

    if not forecast_files:
        print("âš ï¸ æ‰¾ä¸åˆ° â‰¤ t çš„æ­·å² forecastï¼Œç•¥éå›æ¸¬")
        return

    forecast_date = asof_date.normalize()

    forecast_name = f"{forecast_date:%Y-%m-%d}_{ticker}_forecast.csv"
    forecast_csv = os.path.join("results", forecast_name)
    
    if not os.path.exists(forecast_csv):
        print(f"âš ï¸ æ‰¾ä¸åˆ° {forecast_name}ï¼Œç•¥éå›æ¸¬")
        return

    forecast_csv = os.path.join("results", forecast_name)

    print(f"ğŸ“„ Backtest ä½¿ç”¨ forecastï¼š{forecast_name}")

    future_df = pd.read_csv(forecast_csv, parse_dates=["date"])

    # === 4ï¸âƒ£ å–æ•¸å€¼ï¼ˆå®Œå…¨å°é½Šäº¤æ˜“èªæ„ï¼‰===
    close_t   = float(real_df.loc[t, "Close"])
    actual_t1 = float(real_df.loc[t1, "Close"])
    pred_t1   = float(future_df.loc[0, "Pred_Close"])

    # === 5ï¸âƒ£ ç•«åœ–è³‡æ–™ã€Œåªç”¨çœŸå¯¦äº¤æ˜“æ—¥ã€===
    trend = real_df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1],
            "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1],
            "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    dx = 0.08
    price_offset = max(0.2, close_t * 0.002)

    ax.text(x_t, close_t + price_offset,
            f"{close_t:.2f}", ha="center", va="bottom", fontsize=18)
    ax.text(x_t + 1 + dx, pred_t1,
            f"Pred {pred_t1:.2f}", ha="left", va="center",
            fontsize=16, color="red")
    ax.text(x_t + 1 + dx, actual_t1,
            f"Actual {actual_t1:.2f}", ha="left", va="center",
            fontsize=16, color="green")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title(f"{ticker} Decision Backtest (t â†’ t+1)")
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8,
        alpha=0.4,
        ha="left",
        va="bottom"
    )

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{t1:%Y-%m-%d}_{ticker}_backtest.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

    # === 6ï¸âƒ£ CSV è¼¸å‡º ===
    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "actual_day": t1.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    out_csv = f"results/{t1:%Y-%m-%d}_{ticker}_backtest.csv"
    bt.to_csv(out_csv, index=False, encoding="utf-8-sig")

# ================= 6M Trend Plotï¼ˆx è»¸ = æœˆï¼‰ =================
def plot_6m_trend_advanced(
    df: pd.DataFrame,
    last_close: float,
    raw_norm_returns: np.ndarray,
    scale_last: float,
    ticker: str,
    asof_date: pd.Timestamp
):
    MONTHS = 6
    DPM = 21

    # =============================
    # 1ï¸âƒ£ ä¸»å‡è¶¨å‹¢ï¼ˆæ¨¡å‹ï¼‰
    # =============================
    # =============================
# 1ï¸âƒ£ ä¸»å‡è¶¨å‹¢ï¼ˆä½é »ï¼Œä¾†è‡ªæ­·å²åƒ¹æ ¼ï¼‰
# =============================
# ç”¨è¿‘ 120 å€‹äº¤æ˜“æ—¥ä¼°è¨ˆã€Œé•·æœŸ driftã€
    log_price = np.log(df["Close"].astype(float))
    ret_ewm = log_price.diff().ewm(span=60).mean()
    
    daily_drift = float(ret_ewm.iloc[-1])
    daily_drift = np.clip(daily_drift, -0.01, 0.01)  # é˜²çˆ†ï¼ˆÂ±1% / dayï¼‰


    # ===== Regime åˆ¤æ–·ï¼ˆPriority 1ï¼‰=====
    atr = last_valid_value(df, "ATR_14", lookback=40)
    rsi = last_valid_value(df, "RSI", lookback=40)
    
    # æ³¢å‹•å¼·åº¦ï¼ˆç›¸å°åƒ¹æ ¼ï¼‰
    vol_regime = atr / last_close if atr else 0.03
    
    # è¶¨å‹¢å¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0~1ï¼‰
    trend_score = 1.0
    
    # 1ï¸âƒ£ é«˜æª”éç†± â†’ drift ä¸å¯ä¿¡
    if rsi and rsi > 75:
        trend_score *= 0.3
    elif rsi and rsi > 65:
        trend_score *= 0.6
    
    # 2ï¸âƒ£ è¶…ä½æ³¢å‹• â†’ åç›¤æ•´
    if vol_regime < 0.015:
        trend_score *= 0.5
    
    # 3ï¸âƒ£ è¶…é«˜æ³¢å‹• â†’ regime ä¸ç©©
    if vol_regime > 0.08:
        trend_score *= 0.7
    
    # æœ€çµ‚èª¿æ•´ drift
    daily_drift *= trend_score

      
    monthly_logret = daily_drift * DPM
    
    trend = []
    p = last_close
    for _ in range(MONTHS):
        p *= np.exp(monthly_logret)
        trend.append(p)
    
    trend = np.array(trend)
    


    # =============================
    # 2ï¸âƒ£ ä¸»é€±æœŸï¼ˆåƒ¹æ ¼ï¼‰
    # =============================
    close = df["Close"].iloc[-180:].values
    close = close - close.mean()

    fft_p = np.fft.rfft(close)
    freq_p = np.fft.rfftfreq(len(close), d=1)
    idx_p = np.argmax(np.abs(fft_p[1:])) + 1
    cycle_p = np.clip(int(round(1 / freq_p[idx_p])), 40, 120)

    # =============================
# 3ï¸âƒ£ å›æª”é€±æœŸï¼ˆæˆäº¤é‡ï¼‰
# =============================
    vol_series = df["Volume"].iloc[-180:].dropna().values
    
    if len(vol_series) < 60:
        cycle_v = 30  # fallback
    else:
        vol_centered = vol_series - vol_series.mean()
    
        fft_v = np.fft.rfft(vol_centered)
        freq_v = np.fft.rfftfreq(len(vol_centered), d=1)
        idx_v = np.argmax(np.abs(fft_v[1:])) + 1
        cycle_v = np.clip(int(round(1 / freq_v[idx_v])), 20, 60)


    # =============================
    # 4ï¸âƒ£ éœ‡ç›ªå¹…åº¦ï¼ˆATR Ã— RSIï¼‰
    # =============================
    atr = last_valid_value(df, "ATR_14", lookback=40)
    if atr is None:
        raise ValueError("âŒ ç„¡å¯ç”¨ ATR_14ï¼ˆæœ€è¿‘ 40 æ—¥çš†ç‚º NaNï¼‰")
    atr_ratio = atr / last_close

    rsi = last_valid_value(df, "RSI", lookback=40)
    rsi_factor = np.clip(abs(rsi - 50) / 50, 0.3, 1.2)

    base_amp = atr_ratio * rsi_factor
    base_amp = np.clip(base_amp, 0.02, 0.18)

    # =============================
    # 5ï¸âƒ£ åˆæˆåƒ¹æ ¼ï¼ˆå¤šé€±æœŸï¼‰
    # =============================
    prices = [last_close]

    for m in range(1, MONTHS + 1):
        phase_p = 2 * np.pi * (m * DPM) / cycle_p
        phase_v = 2 * np.pi * (m * DPM) / cycle_v

        cycle_main = base_amp * np.sin(phase_p)
        cycle_pull = 0.6 * base_amp * np.sin(phase_v + np.pi)

        price = trend[m - 1] * (1 + cycle_main + cycle_pull)
        prices.append(price)

    prices = np.array(prices)

    # =============================
    # 6ï¸âƒ£ å€é–“å¸¶ï¼ˆATR-based fanï¼‰
    # =============================
    time_scale = np.linspace(0.6, 1.3, len(prices))
    upper = prices * (1 + base_amp * time_scale)
    lower = prices * (1 - base_amp * time_scale)


    # =============================
    # 7ï¸âƒ£ X è»¸ï¼ˆæœˆï¼‰
    # =============================
    labels = ["Now"] + pd.date_range(
        asof_date + pd.offsets.MonthBegin(1),
        periods=MONTHS,
        freq="MS"
    ).strftime("%Y-%m").tolist()

    # =============================
    # 8ï¸âƒ£ Plot
    # =============================
    plt.figure(figsize=(15, 7))
    x = np.arange(MONTHS + 1)

    plt.fill_between(x, lower, upper, alpha=0.18, label="Expected Range")
    plt.plot(x, prices, "r-o", linewidth=2.8, label="Projected Path")
    plt.scatter(0, prices[0], s=180, marker="*", label="Today")

    for i, p in enumerate(prices[1:]):
        plt.text(i + 1, p, f"{p:.2f}", ha="center", fontsize=12)

    plt.xticks(x, labels, fontsize=13)
    plt.title(f"{ticker} Â· 6M Outlook (Multi-Cycle + ATR + RSI)")
    plt.grid(alpha=0.3)
    plt.legend()

    os.makedirs("results", exist_ok=True)
    out = f"results/{datetime.now():%Y-%m-%d}_{ticker}_6m_advanced.png"
    plt.savefig(out, dpi=300, bbox_inches="tight")
    plt.close()

def last_valid_value(df: pd.DataFrame, col: str, lookback: int = 30):
    """
    å–æœ€è¿‘ä¸€ç­†æœ‰æ•ˆï¼ˆé NaNï¼‰çš„æŒ‡æ¨™å€¼
    - ç”¨æ–¼éäº¤æ˜“æ—¥ / è£œ today row çš„æƒ…æ³
    """
    if col not in df.columns:
        return None

    s = df[col].iloc[-lookback:]
    s = s[s.notna()]
    if s.empty:
        return None
    return float(s.iloc[-1])



# ================= Main =================
if __name__ == "__main__":
    TICKER = "2408.TW"
    COLLECTION = "NEW_stock_data_liteon"

    # âœ… è¯æ±å°ˆå±¬è¨­å®šï¼ˆnormalized return ç‰ˆæœ¬ï¼‰
    STOCK_CONFIG = {
        "2408.TW": {
            "LOOKBACK": 40,
            "STEPS": 5,
            "MAX_DAILY_NORMRET": 3.0,
            "LR": 6e-4,
            "LSTM_UNITS": 64
        },
    }

    cfg = STOCK_CONFIG.get(TICKER, {
        "LOOKBACK": 40,
        "STEPS": 5,
        "MAX_DAILY_NORMRET": 3.0,
        "LR": 6e-4,
        "LSTM_UNITS": 64
    })

    LOOKBACK = cfg["LOOKBACK"]
    STEPS = cfg["STEPS"]

    os.makedirs("models", exist_ok=True)
    MODEL_PATH = f"models/{TICKER}_attn_lstm.keras"

    # ---------- Data ----------
    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    df = ensure_latest_trading_row(df)
    df = add_features(df)

    FEATURES = [
        "Close",
        "Volume",
        "RSI",
        "MACD",
        "K",
        "D",
        "ATR_14"
    ]



    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(
            f"âš ï¸ Firestore è³‡æ–™ç¼ºæ¬„ä½ï¼š{missing}\n"
            f"è«‹ç¢ºèª catch_stock.py å¯«å› 8110.TW æ™‚åŒ…å« Open/High/Low/Close/Volumeï¼Œä¸”æŒ‡æ¨™æ¬„ä½å·²å¯«å…¥ã€‚"
        )

    # RET_STD_20 æ˜¯ y çš„å°ºåº¦ï¼Œéœ€è¦ä¸€èµ·å­˜åœ¨ï¼ˆadd_features æœƒåšï¼‰
    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    df = df.dropna()

    X, y_ret, y_dir, idx = create_sequences(df, FEATURES, steps=STEPS, window=LOOKBACK)
    print(f"df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 40:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<40ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK/STEPS æˆ–æª¢æŸ¥è³‡æ–™æ˜¯å¦ç¼ºæ¬„ä½/éå¤š NaNã€‚")

    split = int(len(X) * 0.85)

    X_tr, X_te = X[:split], X[split:]
    y_ret_tr, y_ret_te = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_te = y_dir[:split], y_dir[split:]
    idx_tr, idx_te = idx[:split], idx[split:]

    # âœ… scaler.fit åƒ…ç”¨ train å€é–“ï¼ˆç”¨ idx_tr çš„æœ€å¾Œæ—¥æœŸç•Œå®šï¼‰
    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    sx = MinMaxScaler()
    sx.fit(df_for_scaler.values)

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_te_s = scale_X(X_te)

    # ---------- Model (å°ˆå±¬) ----------
    if os.path.exists(MODEL_PATH):
        print(f"âœ… è¼‰å…¥æ—¢æœ‰æ¨¡å‹ï¼š{MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    else:
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_logret=cfg["MAX_DAILY_NORMRET"]
        )
        model = compile_model(
          model,
          direction_weight=0.8,
          lr=cfg["LR"]
        )

    model.fit(
        X_tr_s,
        {"return": y_ret_tr, "direction": y_dir_tr},
        epochs=80,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ å·²å„²å­˜æ¨¡å‹ï¼š{MODEL_PATH}")

    pred_ret, pred_dir = model.predict(X_te_s, verbose=0)
    raw_norm_returns = pred_ret[-1]  # âœ… normalized returnsï¼ˆå·²é™å¹…ï¼‰

    print(f"ğŸ“ˆ é æ¸¬æ–¹å‘æ©Ÿç‡ï¼ˆçœ‹æ¼²ï¼‰: {pred_dir[-1][0]:.2%}")

    asof_date, is_today_trading = get_asof_trading_day(df)

    if not is_today_trading:
        print(f"â„¹ï¸ ä»Šæ—¥éäº¤æ˜“æ—¥ï¼Œ8110.TW ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥ {asof_date.date()}")
    
    last_close = float(df.loc[asof_date, "Close"])


    # âœ… æŠŠ normalized return ä¹˜å›æ³¢å‹•å°ºåº¦ï¼ˆç”¨ asof çš„ RET_STD_20ï¼‰
    scale_last = float(df.loc[asof_date, "RET_STD_20"])
    if not np.isfinite(scale_last) or scale_last <= 0:
        # fallbackï¼šç”¨æœ€è¿‘ 20 å¤© std ä¼°
        scale_last = float(np.log(df["Close"].astype(float)).diff().rolling(20).std().iloc[-1])
    scale_last = max(scale_last, 1e-6)


    # ğŸ”§ ADD: Regime-based æ³¢æ®µæ”¾å¤§ / å£“ç¸®ï¼ˆç”¨æœ€è¿‘çš„ TREND_60ï¼‰
    trend60 = last_valid_value(df, "TREND_60", lookback=5)
    
    amp = 1.0
    if trend60 is not None:
        if trend60 > 1.0:
            amp = 1.2
        elif trend60 < -1.0:
            amp = 1.1
        else:
            amp = 0.8    # ç›¤æ•´ â†’ å£“ç¸®
    
    print(f"ğŸ“Š Regime amp = {amp:.2f}")

    prices = []
    price = last_close
    for r_norm in raw_norm_returns:
        r = float(r_norm) * scale_last * amp
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    future_df["date"] = pd.bdate_range(
        start=asof_date + BDay(1),
        periods=STEPS
    )


    # âœ… é æ¸¬æ•¸å€¼è¼¸å‡º CSVï¼ˆæª”åå« tickerï¼‰
    os.makedirs("results", exist_ok=True)
    forecast_csv = f"results/{asof_date:%Y-%m-%d}_{TICKER}_forecast.csv"
    future_df.to_csv(forecast_csv, index=False, encoding="utf-8-sig")

    # âœ… åœ–è¼¸å‡ºï¼ˆå…§å®¹ä¸å‹•ã€æª”åæ”¹å« tickerï¼‰
    plot_and_save(df, future_df, ticker=TICKER)
    plot_backtest_error(df, ticker=TICKER)
    # ================= 6M Trend Forecastï¼ˆx è»¸ = æœˆï¼‰ =================
    plot_6m_trend_advanced(
        df=df,
        last_close=last_close,
        raw_norm_returns=raw_norm_returns,
        scale_last=scale_last,
        ticker=TICKER,
        asof_date=asof_date
    )

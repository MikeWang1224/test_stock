# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py (2408.TW å—äºç§‘ï½œæ–¹å‘æ›´æº–ç‰ˆ + æ›´ç©©ç‰ˆ)

ä½ è¦çš„ã€Œæ¨¡å‹ç«¯ã€é‡é»æ”¹å‹•ï¼ˆæœ€å°‘ä½†æœ€æœ‰æ„Ÿï¼‰ï¼š
1) âœ… åŠ å…¥æ™‚é–“åº validationï¼ˆEarlyStopping ç›£çœ‹ val_lossï¼Œä¸å†å‡ç©©ï¼‰
2) âœ… direction æ”¹ç”¨ Focal lossï¼ˆæˆ– TF ä¸æ”¯æ´æ™‚ fallback æˆåŠ æ¬Š BCEï¼‰
3) âœ… direction head èˆ‡ return head å°é½Šï¼šæŠŠã€Œsum(raw_returns)ã€åŠ åˆ°æ–¹å‘ logitï¼ˆé¿å…ä¸€å€‹èªªæ¼²ä¸€å€‹èªªè·Œï¼‰
4) âœ… scaler å­˜æª”/è¼‰å…¥ï¼ˆçºŒè¨“ä¸å†æ¯å¤©æ›åº§æ¨™ç³»ï¼‰
5) âœ… cap å¯«å…¥ meta.jsonï¼šçºŒè¨“æ™‚æ²¿ç”¨åŒä¸€å€‹ capï¼ˆé¿å…æ¨¡å‹åœ–è£¡ cap å›ºå®šå»ä»¥ç‚ºæ›´æ–°äº†ï¼‰

âœ… NEWï¼šæŠŠ Firestore çš„å¤–ç”Ÿå› å­åŠ å…¥æ¨¡å‹ï¼ˆä¸æ”¹ Firestore ä»»ä½•è³‡æ–™ä½ç½®ï¼‰
- TAIEX / ELECTRONICS / USD_TWDï¼šåŒæ—¥å°é½Š
- SOX / MU_USï¼šä»¥ã€Œç¾è‚¡æ”¶ç›¤ -> å°è‚¡ä¸‹ä¸€å€‹äº¤æ˜“æ—¥ã€æ–¹å¼å°é½Šï¼ˆindex + BDay(1)ï¼‰

âš ï¸ åœ–è¡¨èˆ‡è¼¸å‡ºæª”åè¦å‰‡ä¸è®Šï¼ˆä½ çš„ results/xxxx æª”æ¡ˆæ ¼å¼ç¶­æŒåŸæ¨£ï¼‰
"""

import os, json, random
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import joblib  # âœ… scaler persistence

import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Softmax, Lambda
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# ================= Seedï¼ˆè®“çµæœæ›´ç©©ã€å¯æ¯”è¼ƒï¼‰ =================
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()
else:
    print("âš ï¸ FIREBASE æœªè¨­å®šï¼ŒFirestore è®€å–å°‡ç„¡è³‡æ–™")

# ================= Firestore è®€å–ï¼ˆå€‹è‚¡ï¼‰ =================
def load_df_from_firestore(ticker, collection="NEW_stock_data_liteon", days=500):
    rows = []
    if db:
        for doc in db.collection(collection).stream():
            p = doc.to_dict().get(ticker)
            if p:
                rows.append({"date": doc.id, **p})

    df = pd.DataFrame(rows)
    if df.empty:
        raise ValueError(f"âš ï¸ Firestore ç„¡è³‡æ–™ï¼š{ticker}")

    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").tail(days).set_index("date")
    return df

# ================= Firestore è®€å–ï¼ˆå¤–ç”Ÿå› å­ Close onlyï¼‰ =================
def load_factor_close_from_firestore(alias, collection="NEW_stock_data_liteon", days=800):
    """
    è®€å– Firestore æ–‡ä»¶ä¸­çš„ {alias: {Close: ...}}ï¼Œå›å‚³ Series(index=date, value=Close)
    alias ä¾‹ï¼šTAIEX / ELECTRONICS / USD_TWD / SOX / MU_US
    """
    rows = []
    if db:
        for doc in db.collection(collection).stream():
            p = doc.to_dict().get(alias)
            if isinstance(p, dict) and "Close" in p:
                rows.append({"date": doc.id, "Close": p["Close"]})

    df = pd.DataFrame(rows)
    if df.empty:
        raise ValueError(f"âš ï¸ Firestore ç„¡è³‡æ–™ï¼š{alias}")

    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").tail(days).set_index("date")
    s = df["Close"].astype(float)
    s.name = alias
    return s

def attach_factors_to_stock_df(df_stock, collection="NEW_stock_data_liteon"):
    """
    df_stock: 2408 çš„ dfï¼ˆindex=å°è‚¡äº¤æ˜“æ—¥ï¼‰
    - å°è‚¡/åŒ¯ç‡å› å­ï¼ˆTAIEX/ELECTRONICS/USD_TWDï¼‰ï¼šç›´æ¥ reindex + ffill + bfill
    - ç¾è‚¡å› å­ï¼ˆSOX/MU_USï¼‰ï¼šæŠŠç¾è‚¡æ—¥æœŸå¾€å¾Œæ¨ 1 å€‹ BDayï¼Œè½åœ¨å°è‚¡ä¸‹ä¸€äº¤æ˜“æ—¥ï¼Œå† reindex + ffill + bfill
    âš ï¸ åªæ”¹ DataFrameï¼ˆè¨˜æ†¶é«”å…§ï¼‰ï¼Œä¸æœƒæ”¹ Firestore ä»»ä½•è³‡æ–™ã€‚
    """
    df_stock = df_stock.copy()
    idx = df_stock.index

    # å°è‚¡/åŒ¯ç‡ï¼šåŒæ—¥å°é½Š
    for a in ["TAIEX", "ELECTRONICS", "USD_TWD"]:
        try:
            s = load_factor_close_from_firestore(a, collection=collection)
            # âœ… é‡è¦ï¼šffill + bfillï¼Œé¿å…ä¸€é–‹å§‹ä¸€ä¸² NaN ç›´æ¥æŠŠæ•´æ®µç æ‰
            df_stock[a] = s.reindex(idx).ffill().bfill()
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ {a}: {e}")
            df_stock[a] = np.nan

    # ç¾è‚¡ï¼šç¾è‚¡ D çš„ Close -> å°è‚¡ D+1
    for a in ["SOX", "MU_US"]:
        try:
            s = load_factor_close_from_firestore(a, collection=collection)
            s_shifted = s.copy()
            s_shifted.index = (s_shifted.index + BDay(1))
            s_shifted.name = a
            # âœ… åŒæ¨£è£œé½Š
            df_stock[a] = s_shifted.reindex(idx).ffill().bfill()
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ {a}: {e}")
            df_stock[a] = np.nan

    return df_stock

# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_today_row(df):
    today = pd.Timestamp(datetime.now().date())
    last_date = df.index.max()
    if last_date < today:
        df.loc[today] = df.loc[last_date]
        print(f"âš ï¸ ä»Šæ—¥ç„¡è³‡æ–™ï¼Œä½¿ç”¨ {last_date.date()} è£œä»Šæ—¥")
    return df.sort_index()


# ================= Feature Engineering =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    # âœ… Volume å°ºåº¦ç©©å®š
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    # åœ–è¡¨ç”¨å‡ç·šï¼ˆä¿æŒä¸è®Šï¼‰
    df["SMA5"] = df["Close"].rolling(5).mean()
    df["SMA10"] = df["Close"].rolling(10).mean()
    return df

# ================= Sequenceï¼ˆé¿å…éŒ¯ä½ï¼Œä¸”ä¸äº‚åˆ‡ dfï¼‰ =================
def create_sequences(df, features, steps=5, window=40):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 çš„ log return
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆsum future_ret > 0ï¼‰
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰çš„ã€Œt ç•¶å¤©æ—¥æœŸã€
    """
    X, y_ret, y_dir, idx = [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()
    feat = df[features].values

    for i in range(window, len(df) - steps):
        x_seq = feat[i - window:i]
        future_ret = logret.iloc[i:i + steps].values

        if np.any(np.isnan(future_ret)) or np.any(np.isnan(x_seq)):
            continue

        X.append(x_seq)
        y_ret.append(future_ret)
        y_dir.append(1.0 if future_ret.sum() > 0 else 0.0)
        idx.append(df.index[i])

    return np.array(X), np.array(y_ret), np.array(y_dir), np.array(idx)

# ================= Lossï¼ˆdirection ç”¨ focalï¼›ä¸æ”¯æ´å°± fallbackï¼‰ =================
def get_direction_loss():
    if hasattr(tf.keras.losses, "BinaryFocalCrossentropy"):
        return tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0)

    def weighted_bce(y_true, y_pred, pos_weight=1.5):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-7, 1.0 - 1e-7)
        bce = -(y_true * tf.math.log(y_pred) + (1.0 - y_true) * tf.math.log(1.0 - y_pred))
        w = y_true * pos_weight + (1.0 - y_true) * 1.0
        return tf.reduce_mean(w * bce)

    return weighted_bce

# ================= Model buildï¼ˆreturn é™å¹… + æ–¹å‘èˆ‡returnå°é½Šï¼‰ =================
def build_attention_lstm(input_shape, steps, max_daily_logret=0.06, dir_from_ret_weight=2.0):
    inp = Input(shape=input_shape)

    x = LSTM(64, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    raw = Dense(steps, activation="tanh", name="raw_returns")(context)
    out_ret = Lambda(lambda t: t * max_daily_logret, name="return")(raw)

    base_logit = Dense(1, activation=None, name="dir_base_logit")(context)
    sum_raw = Lambda(lambda r: tf.reduce_sum(r, axis=1, keepdims=True), name="sum_raw")(raw)
    dir_logit = Lambda(lambda t: t[0] + dir_from_ret_weight * t[1], name="dir_logit")([base_logit, sum_raw])
    out_dir = Lambda(lambda z: tf.sigmoid(z), name="direction")(dir_logit)

    model = Model(inp, [out_ret, out_dir])
    return model

def compile_model(model, direction_weight=0.8, lr=7e-4):
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": get_direction_loss()
        },
        loss_weights={
            "return": 1.0,
            "direction": float(direction_weight)
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆToday æ¨™è¨˜ï¼Œæª”ååŠ  tickerï¼‰ =================
def plot_and_save(df_hist, future_df, ticker):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18, 8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}", fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}", color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title(f"{ticker} Attention-LSTM é æ¸¬")

    os.makedirs("results", exist_ok=True)
    plt.savefig(f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png", dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼Œè®€å°æ‡‰ ticker forecastï¼‰ =================
def plot_backtest_error(df, ticker):
    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    # === æ‰¾æœ€è¿‘ä¸€ä»½ã€Œå·²ç™¼ç”Ÿã€çš„ forecast ===
    suffix = f"_{ticker}_forecast.csv"
    forecast_files = []

    for f in os.listdir("results"):
        if not f.endswith(suffix):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
            forecast_files.append((d, f))
        except Exception:
            continue

    if not forecast_files:
        print(f"âš ï¸ æ‰¾ä¸åˆ° forecastï¼š{ticker}")
        return

    forecast_files.sort(key=lambda x: x[0], reverse=True)
    forecast_date, forecast_name = forecast_files[0]
    future_df = pd.read_csv(
        os.path.join("results", forecast_name),
        parse_dates=["date"]
    )

    # === åªç”¨çœŸå¯¦äº¤æ˜“æ—¥ ===
    t, t1 = get_last_two_trading_days(df)

    close_t = float(df.loc[t, "Close"])
    actual_t1 = float(df.loc[t1, "Close"])

    # forecast çš„ç¬¬ä¸€å¤©å¿…é ˆæ˜¯ t1
    pred_row = future_df[future_df["date"] == t1]
    if pred_row.empty:
        print("âš ï¸ forecast èˆ‡äº¤æ˜“æ—¥æœªå°é½Šï¼Œç•¥éå›æ¸¬")
        return

    pred_t1 = float(pred_row["Pred_Close"].iloc[0])

    # === ç¹ªåœ– ===
    trend = df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1],
            "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1],
            "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    price_offset = max(0.2, close_t * 0.002)

    ax.text(x_t, close_t + price_offset, f"{close_t:.2f}",
            ha="center", fontsize=18)
    ax.text(x_t + 1.05, pred_t1, f"Pred {pred_t1:.2f}",
            color="red", fontsize=16, va="center")
    ax.text(x_t + 1.05, actual_t1, f"Actual {actual_t1:.2f}",
            color="green", fontsize=16, va="center")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title(f"{ticker} Decision Backtest (t â†’ t+1)")
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8, alpha=0.4
    )

    os.makedirs("results", exist_ok=True)
    today = datetime.now().date()
    plt.savefig(f"results/{today}_{ticker}_backtest.png",
                dpi=300, bbox_inches="tight")
    plt.close()

    # === CSV ===
    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    bt.to_csv(
        f"results/{today}_{ticker}_backtest.csv",
        index=False,
        encoding="utf-8-sig"
    )


def get_last_two_trading_days(df):
    """
    å›å‚³æœ€å¾Œå…©å€‹ã€ŒçœŸå¯¦äº¤æ˜“æ—¥ã€ (t, t+1)
    """
    idx = df.index.sort_values()
    if len(idx) < 2:
        raise ValueError("âš ï¸ äº¤æ˜“æ—¥ä¸è¶³ï¼Œç„¡æ³•å›æ¸¬")
    return idx[-2], idx[-1]

# ================= Main =================
if __name__ == "__main__":
    TICKER = "2408.TW"
    LOOKBACK = 40
    STEPS = 5
    COLLECTION = "NEW_stock_data_liteon"

    os.makedirs("results", exist_ok=True)
    MODEL_PATH  = f"results/{TICKER}_model.keras"
    SCALER_PATH = f"results/{TICKER}_scaler.pkl"
    META_PATH   = f"results/{TICKER}_meta.json"

    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    #df = ensure_today_row(df)
    df = add_features(df)

    # âœ… NEWï¼šæ¥å¤–ç”Ÿå› å­ï¼ˆåªæ”¹ DataFrameï¼Œä¸æ”¹ Firestoreï¼‰
    df = attach_factors_to_stock_df(df, collection=COLLECTION)

    FEATURES = [
        "Close", "Volume", "RSI", "MACD", "K", "D", "ATR_14",
        "TAIEX", "ELECTRONICS", "USD_TWD", "SOX", "MU_US"
    ]

    cols_check = [c for c in ["Close", "TAIEX", "ELECTRONICS", "USD_TWD", "SOX", "MU_US"] if c in df.columns]
    print("ğŸ” factors tail:\n", df[cols_check].tail(5))

    # âœ… é—œéµä¿®æ­£ 1ï¼šä¸è¦æ•´å¼µ df.dropna()ï¼Œåªé‡å°æ¨¡å‹ FEATURES
    df = df.dropna(subset=FEATURES)

    X, y_ret, y_dir, idx = create_sequences(df, FEATURES, steps=STEPS, window=LOOKBACK)
    print(f"{TICKER} | df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 40:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<40ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK/STEPS æˆ–æª¢æŸ¥è³‡æ–™æ˜¯å¦ç¼ºæ¬„ä½/éå¤š NaNã€‚")

    split = int(len(X) * 0.85)

    X_tr, X_te = X[:split], X[split:]
    y_ret_tr, y_ret_te = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_te = y_dir[:split], y_dir[split:]
    idx_tr, idx_te = idx[:split], idx[split:]

    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    if os.path.exists(SCALER_PATH):
        sx = joblib.load(SCALER_PATH)
        print(f"âœ… Load scaler: {SCALER_PATH}")
    else:
        sx = MinMaxScaler()
        sx.fit(df_for_scaler.values)
        joblib.dump(sx, SCALER_PATH)
        print(f"ğŸ’¾ Scaler saved: {SCALER_PATH}")

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_te_s = scale_X(X_te)

    train_close = df.loc[:train_end_date, "Close"].astype(float)
    train_logret_abs = np.log(train_close).diff().dropna().abs()

    auto_cap = float(train_logret_abs.quantile(0.99))
    auto_cap = float(np.clip(auto_cap, 0.03, 0.10))
    print(f"âœ… max_daily_logret auto (99% quantile, clipped): {auto_cap:.4f}")

    meta = {}
    if os.path.exists(META_PATH):
        try:
            with open(META_PATH, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except Exception:
            meta = {}

    if "cap" in meta:
        cap_used = float(meta["cap"])
        if abs(cap_used - auto_cap) > 1e-6:
            print(f"âš ï¸ cap å·²å›ºå®šæ²¿ç”¨ meta cap={cap_used:.4f}ï¼ˆauto_cap={auto_cap:.4f} ä¸å¥—ç”¨ï¼‰")
    else:
        cap_used = auto_cap
        meta = {
            "ticker": TICKER,
            "lookback": LOOKBACK,
            "steps": STEPS,
            "features": FEATURES,
            "cap": cap_used,
            "created_at_tw": f"{now_tw:%Y-%m-%d %H:%M:%S}"
        }
        with open(META_PATH, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Meta saved: {META_PATH} (cap={cap_used:.4f})")

    DIRECTION_WEIGHT = 0.8

    n_tr = len(X_tr_s)
    val_cut = int(n_tr * 0.90)
    if val_cut < 10:
        raise ValueError("âš ï¸ train å¤ªå°‘ï¼Œç„¡æ³•åˆ‡ valã€‚è«‹å¢åŠ è³‡æ–™æˆ–é™ä½ LOOKBACK/STEPSã€‚")

    X_fit, X_val = X_tr_s[:val_cut], X_tr_s[val_cut:]
    y_ret_fit, y_ret_val = y_ret_tr[:val_cut], y_ret_tr[val_cut:]
    y_dir_fit, y_dir_val = y_dir_tr[:val_cut], y_dir_tr[val_cut:]

    print(f"âœ… Fit samples: {len(X_fit)} | Val samples: {len(X_val)}")

    if os.path.exists(MODEL_PATH):
        print(f"âœ… Load existing model: {MODEL_PATH}")
        model = load_model(MODEL_PATH, safe_mode=False)
        model = compile_model(model, direction_weight=DIRECTION_WEIGHT, lr=3e-4)
    else:
        print("âœ… Build new model")
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_logret=cap_used,
            dir_from_ret_weight=2.0
        )
        model = compile_model(model, direction_weight=DIRECTION_WEIGHT, lr=7e-4)

    model.fit(
        X_fit,
        {"return": y_ret_fit, "direction": y_dir_fit},
        validation_data=(X_val, {"return": y_ret_val, "direction": y_dir_val}),
        epochs=80,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ Model saved: {MODEL_PATH}")

    pred_ret, pred_dir = model.predict(X_te_s, verbose=0)
    raw_returns = pred_ret[-1]

    print(f"ğŸ“ˆ {TICKER} é æ¸¬æ–¹å‘æ©Ÿç‡ï¼ˆçœ‹æ¼²ï¼‰: {pred_dir[-1][0]:.2%}")

    asof_date = df.index.max()
    last_close = float(df.loc[asof_date, "Close"])

    prices = []
    price = last_close
    for r in raw_returns:
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    last_trade_date = df.index[-1]

    # ================= ç”Ÿæˆæœªä¾†äº¤æ˜“æ—¥ï¼ˆå°è‚¡å¯¦éš›äº¤æ˜“æ—¥ï¼‰ =================
    # å¾ df index æ‰¾åˆ° asof_date çš„ä½ç½®
    asof_idx = df.index.get_loc(asof_date)
    future_dates = df.index[asof_idx + 1 : asof_idx + 1 + STEPS]
    
    # è‹¥è³‡æ–™ä¸è¶³ STEPS å¤©ï¼Œè£œæœ€å¾Œä¸€å¤©ï¼ˆé¿å…å ±éŒ¯ï¼‰
    if len(future_dates) < STEPS:
        last_date = df.index[-1]
        while len(future_dates) < STEPS:
            future_dates = future_dates.append(pd.DatetimeIndex([last_date]))
    
    future_df["date"] = future_dates



    future_df.to_csv(
        f"results/{datetime.now():%Y-%m-%d}_{TICKER}_forecast.csv",
        index=False,
        encoding="utf-8-sig"
    )

    plot_and_save(df, future_df, TICKER)
    plot_backtest_error(df, TICKER)

#8110stock 


# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py  (8110stock.py)
- Attention-LSTM 
- Multi-task: Return path + Direction
- âœ… å°è³‡æ–™å‹å–„ç‰ˆï¼šæ›´ç©©ã€æ›´ä¸å®¹æ˜“äº‚å™´
  1) LOOKBACK=40, STEPS=5
  2) LSTM + Attention poolingï¼ˆåƒæ•¸æ¯” Transformer æ›´é©åˆå°è³‡æ–™ï¼‰
  3) âœ… Return head åŠ  tanh é™å¹…ï¼ˆé¿å…é æ¸¬çˆ†ç‚¸ï¼‰
  4) âœ… Volume åš log1pï¼ˆå°è³‡æ–™æ›´ç©©ï¼‰ 
- åœ–è¡¨è¼¸å‡ºå®Œå…¨ä¸è®Šï¼ˆä¿ç•™ Today æ¨™è¨˜ï¼‰

âœ… æ”¹1ï¼šä¿®æ­£ scaler fit / split åº§æ¨™ç³»ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼ˆleakageï¼‰
  - create_sequences å›å‚³æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æ—¥æœŸ idx
  - split ç”¨æ¨£æœ¬æ•¸åˆ‡ï¼Œscaler.fit åªç”¨ train å€é–“çš„ df ç‰¹å¾µ

âœ… æ–°å¢ï¼šåŒæ™‚è¼¸å‡º PNG + CSVï¼ˆæª”åå« tickerï¼‰
  - results/YYYY-MM-DD_TICKER_pred.png
  - results/YYYY-MM-DD_TICKER_forecast.csv
  - results/YYYY-MM-DD_TICKER_backtest.png
  - results/YYYY-MM-DD_TICKER_backtest.csv

âœ… è¯æ± 8110.TW å°ˆå±¬å¼·åŒ–ï¼ˆç…§å‰é¢å»ºè­°æ”¹ï¼‰
  A) Featureï¼šåŠ å…¥ HL_RANGE / GAP / VOL_RELï¼ˆæ›´è²¼è¿‘ä¸­å°å‹è‚¡/æ³¢å‹•è‚¡ï¼‰
  B) Targetï¼šé æ¸¬ã€Œæ³¢å‹•æ¨™æº–åŒ–ã€log-returnï¼ˆç”¨ t-1 çš„ RET_STD_20 åšå°ºåº¦ï¼Œé¿å…å·çœ‹ï¼‰
  C) æ¨å›åƒ¹æ ¼æ™‚ï¼šæŠŠé æ¸¬çš„ normalized return ä¹˜å› asof çš„ RET_STD_20
  D) loss_weightsï¼šdirection æ¬Šé‡æé«˜ï¼ˆæ–¹å‘é€šå¸¸æ¯”ç²¾æº–åƒ¹æ›´å¯é ï¼‰
"""

import os, json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout,
    Softmax, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()

# ================= Firestore è®€å– =================
def load_df_from_firestore(
    ticker,
    collection="NEW_stock_data_liteon",
    days=500
):
    if db is None:
        raise ValueError("âŒ Firestore æœªåˆå§‹åŒ–")

    rows = []

    for doc in db.collection(collection).stream():
        p = doc.to_dict().get(ticker)
        if p:
            rows.append({
                "date": doc.id,   # YYYY-MM-DD
                **p
            })

    if not rows:
        raise ValueError("âš ï¸ Firestore ç„¡è³‡æ–™")

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["date"])

    # âœ… é€™è£¡æ‰æ˜¯ã€Œé˜²å‡æ—¥çš„ç¬¬ä¸€é“é–€ã€
    df = (
        df.sort_values("date")
          .tail(days)          # åªä¿ç•™æœ€è¿‘ N ç­†ã€Œäº¤æ˜“æ—¥ã€
          .set_index("date")
    )
    return df



# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_latest_trading_row(df):
    """
    è‹¥ä»Šå¤©æ˜¯éäº¤æ˜“æ—¥ï¼Œè£œ rowï¼ˆforward fillï¼‰
    ä½† Close ä¸æœƒè®Šï¼Œç”¨æ–¼ã€Œé æ¸¬ today+1ã€
    """
    today = pd.Timestamp(datetime.now().date())
    last = df.index.max()

    if last.normalize() >= today:
        return df

    all_days = pd.bdate_range(last, today)

    for d in all_days[1:]:
        if d not in df.index:
            df.loc[d] = df.loc[last]

    return df.sort_index()


def get_asof_trading_day(df: pd.DataFrame):
    """
    å›å‚³ (asof_date, is_today_trading)
    - è‹¥ä»Šå¤©æ˜¯äº¤æ˜“æ—¥ â†’ ç”¨ä»Šå¤©
    - è‹¥ä»Šå¤©éäº¤æ˜“æ—¥ â†’ ç”¨æœ€è¿‘ä¸€å€‹äº¤æ˜“æ—¥
    """
    today = pd.Timestamp(datetime.now().date())
    last_trading_day = df.index.max()

    if last_trading_day.normalize() == today:
        return last_trading_day, True
    else:
        return last_trading_day, False



# ================= Feature Engineeringï¼ˆè¯æ±å°ˆå±¬ï¼‰ =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # âœ… Volume å°ºåº¦ç©©å®šï¼ˆéå¸¸å»ºè­°ï¼‰
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    # åœ–è¡¨ç”¨å‡ç·šï¼ˆä¿æŒä¸è®Šï¼‰
    df["SMA5"] = df["Close"].rolling(5).mean()
    df["SMA10"] = df["Close"].rolling(10).mean()

    # âœ… è¯æ±ï¼ˆæ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰ç‰¹å¾µ
    # éœ€è¦ Firestore æœ‰ Open/High/Lowï¼ˆä½  catch_stock.py å¯«å…¥æ˜¯æœ‰çš„ï¼‰
    if all(c in df.columns for c in ["Open", "High", "Low", "Close"]):
        df["HL_RANGE"] = (df["High"].astype(float) - df["Low"].astype(float)) / df["Close"].astype(float)
        df["GAP"] = (df["Open"].astype(float) - df["Close"].shift(1).astype(float)) / df["Close"].shift(1).astype(float)
    else:
        # è‹¥ç¼ºæ¬„ä½ï¼Œå…ˆçµ¦ NaNï¼ˆå¾Œé¢ dropna æœƒæ’æ‰ï¼‰
        df["HL_RANGE"] = np.nan
        df["GAP"] = np.nan

    # âœ… é‡èƒ½ç›¸å°å¼·å¼±ï¼ˆç”¨ log1p ä¹‹å¾Œçš„ Volume å»åšæ¯”å€¼å³å¯ï¼‰
    df["VOL_REL"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)

    # âœ… 20æ—¥æ³¢å‹•ï¼ˆç”¨ä¾†æ¨™æº–åŒ– yï¼‰
    close = df["Close"].astype(float)
    df["RET_STD_20"] = np.log(close).diff().rolling(20).std()

    # ğŸ”§ ADD: Regime / æ³¢æ®µç‹€æ…‹ç‰¹å¾µï¼ˆä¸å­˜ Firebaseï¼‰
    ma60 = df["Close"].rolling(60)
    df["TREND_60"] = (df["Close"] - ma60.mean()) / (ma60.std() + 1e-9)
    
    df["TREND_SLOPE_20"] = (
        df["Close"].rolling(20).mean().diff()
    ) / df["Close"]
    
    return df


# ================= Sequenceï¼ˆæ¨™æº–åŒ– returnï¼Œé¿å…æ³¢å‹• regime å½±éŸ¿ï¼‰ =================
def create_sequences(
    df, features,
    steps=5, window=40,
    trend_h=20,           # âœ… æ–°å¢ï¼šè¶¨å‹¢ horizonï¼ˆäº¤æ˜“æ—¥ï¼‰
    k_flat=0.8,           # âœ… æ–°å¢ï¼šç›¤æ•´é–€æª»ï¼ˆè¶Šå¤§è¶Šä¿å®ˆï¼‰
    eps=1e-9
):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 normalized log return (ç”¨ t-1 æ³¢å‹•åšå°ºåº¦)
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆäºŒåˆ†é¡ï¼Œä¿ç•™çµ¦çŸ­ç·šï¼‰
    y_trend3: æœªä¾† trend_h å¤©è¶¨å‹¢ï¼ˆä¸‰åˆ†é¡ Up/Flat/Downï¼‰âœ… æ›´è²¼è¿‘çœŸå¯¦
      - ç”¨æ³¢å‹•é–€æª»ï¼š|cumret| < k_flat * scale * sqrt(trend_h) => Flat
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰ t ç•¶å¤©æ—¥æœŸ
    """
    X, y_ret, y_dir, y_trend3, idx = [], [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    feat = df[features].values

    # éœ€è¦åŒæ™‚æ»¿è¶³ steps èˆ‡ trend_h çš„æœªä¾†è³‡æ–™
    max_h = max(steps, trend_h)

    for i in range(window, len(df) - max_h):
        x_seq = feat[i - window:i]
        if np.any(np.isnan(x_seq)):
            continue

        # âœ… ç”¨ t-1 æ³¢å‹•å°ºåº¦ï¼ˆé¿å…å·çœ‹ï¼‰
        scale = df["RET_STD_20"].iloc[i - 1]
        if pd.isna(scale) or scale < eps:
            continue
        scale = float(scale) + eps

        # ---------- 5D return head ----------
        future_ret_raw_5d = logret.iloc[i:i + steps].values
        if np.any(np.isnan(future_ret_raw_5d)):
            continue
        future_ret_norm_5d = future_ret_raw_5d / scale

        # çŸ­ç·šæ–¹å‘ï¼ˆäºŒåˆ†é¡ï¼Œä¿ç•™ï¼‰
        dir_5d = 1.0 if future_ret_raw_5d.sum() > 0 else 0.0

        # ---------- 20D trend head (3-class) ----------
        future_ret_raw_tr = logret.iloc[i:i + trend_h].values
        if np.any(np.isnan(future_ret_raw_tr)):
            continue

        cum = float(future_ret_raw_tr.sum())  # logç´¯ç©
        # âœ… ç›¤æ•´é–€æª»ï¼šæ³¢å‹• * sqrt(h)
        thr = float(k_flat) * scale * np.sqrt(float(trend_h))

        # class: 0=Down, 1=Flat, 2=Up
        if cum > thr:
            cls = 2
        elif cum < -thr:
            cls = 0
        else:
            cls = 1

        onehot = np.zeros(3, dtype=np.float32)
        onehot[cls] = 1.0

        X.append(x_seq)
        y_ret.append(future_ret_norm_5d)
        y_dir.append(dir_5d)
        y_trend3.append(onehot)
        idx.append(df.index[i])

    return (
        np.array(X),
        np.array(y_ret),
        np.array(y_dir),
        np.array(y_trend3),
        np.array(idx)
    )

def build_attention_lstm(
    input_shape,
    steps,
    max_daily_normret=3.0,
    learning_rate=6e-4,
    lstm_units=64
):
    inp = Input(shape=input_shape)

    x = LSTM(lstm_units, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    # âœ… return headï¼štanh é™å¹…ï¼ˆnormalized returnï¼‰
    raw = Dense(steps, activation="tanh")(context)           # [-1, 1]
    out_ret = Lambda(lambda t: t * max_daily_normret, name="return")(raw)

    # âœ… 5D directionï¼ˆçŸ­ç·šï¼‰
    out_dir = Dense(1, activation="sigmoid", name="direction")(context)

    # âœ… 20D trendï¼ˆä¸‰åˆ†é¡ï¼šDown/Flat/Upï¼‰â†’ æ›´è²¼è¿‘çœŸå¯¦
    out_trend = Dense(3, activation="softmax", name="trend3")(context)

    model = Model(inp, [out_ret, out_dir, out_trend])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": "binary_crossentropy",
            "trend3": "categorical_crossentropy"
        },
        # âœ… è¶¨å‹¢æ¯”çŸ­ç·šæ–¹å‘æ›´é‡è¦ï¼ˆæ›´è²¼è¿‘çœŸå¯¦ï¼‰
        loss_weights={
            "return": 1.0,
            "direction": 0.4,
            "trend3": 1.2
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ],
            "trend3": [
                tf.keras.metrics.CategoricalAccuracy(name="acc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆå…§å®¹ä¸å‹•ï¼šæ–°å¢ Today æ¨™è¨˜ï¼‰ =================
def plot_and_save(df_hist, future_df, ticker: str):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18,8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    # âœ… Today é»èˆ‡æ–‡å­—ï¼ˆhist æœ€å¾Œä¸€å€‹é»ï¼‰
    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}",
            fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}",
                color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title(f"{ticker} Attention-LSTM é æ¸¬") # âœ… å…§å®¹ä¸å‹•

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
def plot_backtest_error(df: pd.DataFrame, ticker: str):
    """
    Decision-based Backtest (t â†’ t+1)
    - åƒ…ä½¿ç”¨ã€Œå·²ç™¼ç”Ÿã€çš„ forecast
    - åš´æ ¼ç”¨äº¤æ˜“æ—¥æ—¥æœŸå°é½Šï¼ˆä¸é  row indexï¼‰
    """

    today = pd.Timestamp(datetime.now().date())

    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    # --------------------------------------------------
    # 1) æ‰¾æœ€è¿‘ä¸€ä»½ã€Œå¯å›æ¸¬ã€çš„ forecastï¼ˆæ’é™¤ä»Šå¤©ï¼‰
    # --------------------------------------------------
    forecast_files = []
    suffix = f"_{ticker}_forecast.csv"

    for f in os.listdir("results"):
        if not f.endswith(suffix):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
        except Exception:
            continue
        if d < today:
            forecast_files.append((d, f))

    if not forecast_files:
        print("âš ï¸ æ‰¾ä¸åˆ°å¯ç”¨çš„æ­·å² forecastï¼ˆå·²æ’é™¤ä»Šå¤©ï¼‰")
        return

    forecast_date, forecast_name = max(forecast_files, key=lambda x: x[0])
    forecast_path = os.path.join("results", forecast_name)

    print(f"ğŸ“„ Backtest ä½¿ç”¨ forecastï¼š{forecast_name}")

    future_df = pd.read_csv(forecast_path, parse_dates=["date"])

    # --------------------------------------------------
    # 2) æ±ºå®š t / t+1ï¼ˆçœŸå¯¦äº¤æ˜“æ—¥ï¼‰
    # --------------------------------------------------
    real_days = df.index[df.index < today]
    if len(real_days) < 2:
        print("âš ï¸ çœŸå¯¦äº¤æ˜“æ—¥ä¸è¶³ï¼Œç•¥éå›æ¸¬")
        return

    t = real_days[-2]
    t1 = real_days[-1]

    close_t = float(df.loc[t, "Close"])
    actual_t1 = float(df.loc[t1, "Close"])

    # --------------------------------------------------
    # 3) å°é½Š forecast çš„ t+1
    # --------------------------------------------------
    row = future_df[future_df["date"] == t1]
    if row.empty:
        print("âš ï¸ forecast èˆ‡äº¤æ˜“æ—¥æœªå°é½Šï¼Œç•¥éå›æ¸¬")
        return

    pred_t1 = float(row["Pred_Close"].iloc[0])

    # --------------------------------------------------
    # 4) ç•«æ±ºç­–åˆ†å²”åœ–
    # --------------------------------------------------
    trend = df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1],
            "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1],
            "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    price_offset = max(0.2, close_t * 0.002)
    ax.text(x_t, close_t + price_offset, f"{close_t:.2f}",
            ha="center", fontsize=18)
    ax.text(x_t + 1.05, pred_t1, f"Pred {pred_t1:.2f}",
            color="red", fontsize=16, va="center")
    ax.text(x_t + 1.05, actual_t1, f"Actual {actual_t1:.2f}",
            color="green", fontsize=16, va="center")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title(f"{ticker} Decision Backtest (t â†’ t+1)")
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8,
        alpha=0.4
    )

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{today:%Y-%m-%d}_{ticker}_backtest.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

    # --------------------------------------------------
    # 5) å›æ¸¬ CSV
    # --------------------------------------------------
    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    out_csv = f"results/{today:%Y-%m-%d}_{ticker}_backtest.csv"
    bt.to_csv(out_csv, index=False, encoding="utf-8-sig")


import glob

def plot_6m_trend_advanced(
    df: pd.DataFrame,
    last_close: float,
    raw_norm_returns: np.ndarray,
    scale_last: float,
    ticker: str,
    asof_date: pd.Timestamp,
    amp: float = 1.0,
    pred_ret_all=None,          # å¯é¸ï¼šå‚³å…¥ pred_ret å…¨éƒ¨ (N, STEPS)
    pred_dir_last=None,         # å¯é¸ï¼šå‚³å…¥æœ€å¾Œä¸€ç­†æ–¹å‘æ©Ÿç‡ (float, 0~1)
    k_ens: int = 20
):
    """
    8110-tuned 6M Outlook (Realistic Trend Forecast)
    âœ… æ›´è²¼è¿‘ç¾å¯¦çš„æ ¸å¿ƒï¼š
      1) drift æœ‰æ³¢å‹•ä¸Šé™ï¼ˆvol-capï¼‰ï¼Œé¿å…å‡è¶¨å‹¢ä¸€è·¯å™´
      2) band ç”¨ã€Œå›æ¸¬ log-return èª¤å·®ã€æ ¡æº–ï¼ˆæ¯”åƒ¹æ ¼å·®æ›´ç©©ï¼‰
      3) conf å½±éŸ¿ band å¯¬åº¦ï¼ˆä¸ç¢ºå®š => band è®Šå¯¬ï¼‰
      4) FFT é€±æœŸåŠ  gateï¼ˆé¿å…å‡é€±æœŸï¼‰
    """
    MONTHS = 6
    DPM = 21
    eps = 1e-9

    # -----------------------------
    # 0) å–æ¨¡å‹ 5æ—¥è¼¸å‡ºï¼ˆensemble æ›´ç©©ï¼‰
    # -----------------------------
    if pred_ret_all is not None:
        try:
            K = min(int(k_ens), len(pred_ret_all))
            base5 = np.median(np.asarray(pred_ret_all)[-K:], axis=0).astype(float)
        except Exception:
            base5 = np.array(raw_norm_returns, dtype=float)
    else:
        base5 = np.array(raw_norm_returns, dtype=float)

    if base5 is None or len(base5) == 0:
        raise ValueError("âŒ base5 ç‚ºç©ºï¼šraw_norm_returns/pred_ret_all ç„¡æ³•ä½¿ç”¨")

    # -----------------------------
    # 1) æ­·å² driftï¼ˆlog-returnï¼‰
    # -----------------------------
    close = df["Close"].astype(float)
    logp = np.log(close + eps)
    ret = logp.diff()

    # ä½¿ç”¨è¼ƒå¹³æ»‘çš„ driftï¼ˆé¿å…å¤ªå™ªï¼‰
    daily_drift = float(ret.ewm(span=60).mean().tail(20).mean())
    daily_drift = float(np.clip(daily_drift, -0.01, 0.01))

    # -----------------------------
    # 2) Regimeï¼šATR / RSI å½±éŸ¿ã€Œè¶¨å‹¢å¯ä¿¡åº¦ã€
    # -----------------------------
    atr = last_valid_value(df, "ATR_14", lookback=40)
    rsi = last_valid_value(df, "RSI", lookback=40)

    if atr is None:
        # fallbackï¼šç”¨æœ€è¿‘æ³¢å‹•è¿‘ä¼¼ ATR%
        vol20 = float(ret.dropna().tail(40).std())
        atr_ratio = float(np.clip(vol20 * np.sqrt(1.0), 0.01, 0.20))
    else:
        atr_ratio = float(atr) / float(last_close + eps)

    if rsi is None:
        rsi = 50.0

    trend_score = 1.0
    if rsi > 75:
        trend_score *= 0.35
    elif rsi > 65:
        trend_score *= 0.65

    # è¶…ä½æ³¢å‹• or è¶…é«˜æ³¢å‹•ï¼Œéƒ½ä¸å¤ªç›¸ä¿¡è¶¨å‹¢ï¼ˆæ›´è²¼è¿‘å¸‚å ´ï¼‰
    if atr_ratio < 0.015:
        trend_score *= 0.6
    if atr_ratio > 0.08:
        trend_score *= 0.75

    # -----------------------------
    # 3) æ¨¡å‹ edgeï¼šåªèª¿ driftï¼ˆä¸åšç¡¬å¤–æ¨è¤‡åˆ©ï¼‰
    # -----------------------------
    edge_daily = float(np.mean(base5)) * float(scale_last)  # è½‰å›ã€Œæ—¥ log-return edgeã€

    # éç†±å£“ edgeï¼ˆæ›´è²¼è¿‘ç¾å¯¦ï¼‰
    if rsi > 75:
        edge_daily *= 0.6

    # å–®æ—¥ edge ä¸Šé™ï¼ˆä½ åŸæœ¬ Â±0.4% å¾ˆåˆç†ï¼‰
    edge_daily = float(np.clip(edge_daily, -0.004, 0.004))

    # drift åˆæˆï¼ˆå†ä¹˜ regimeï¼‰
    daily_drift_adj = (daily_drift + edge_daily) * trend_score
    daily_drift_adj = float(np.clip(daily_drift_adj, -0.01, 0.01))

    # âœ… æœˆ driftï¼ˆlog spaceï¼‰
    monthly_logret = daily_drift_adj * DPM

    # âœ… ç‰©ç†ç´„æŸï¼šæœˆ drift ä¸å¯è¶…éã€Œæ³¢å‹•å°ºåº¦çš„å¹¾å€ã€
    # å¸¸è¦‹åˆç†ä¸Šé™ï¼š~ 1.2~1.6 * ATR% * sqrt(21)
    vol_cap = float(1.35 * atr_ratio * np.sqrt(DPM))

    # âœ… RSI éç†±ï¼šæœˆè¶¨å‹¢ä¸Šé™ç¸®å°ï¼ˆæ›´åƒçœŸå¯¦ï¼‰
    if rsi > 75:
        vol_cap *= 0.55
    elif rsi > 65:
        vol_cap *= 0.75
    
    vol_cap = float(np.clip(vol_cap, 0.03, 0.18))  # ä¸Šé™å¾ 0.25 æ”¶åˆ° 0.18
      # 8110 ä¿å®ˆäº›ï¼šæœˆæœ€å¤§ç´„ 3%~25%ï¼ˆlogï¼‰
    monthly_logret = float(np.clip(monthly_logret, -vol_cap, vol_cap))

    model_1m_price = float(last_close * np.exp(monthly_logret))

    # -----------------------------
    # 4) FFT é€±æœŸï¼šåŠ  gate é¿å…å‡é€±æœŸ
    # -----------------------------
    def pick_cycle_from_fft(x, lo, hi, fallback):
        x = np.asarray(x, dtype=float)
        if len(x) < 80:
            return fallback

        xc = x - np.mean(x)
        fft = np.fft.rfft(xc)
        mag = np.abs(fft)
        mag[0] = 0.0

        # top1 / top2 gate
        order = np.argsort(mag)[::-1]
        if len(order) < 3:
            return fallback

        top1 = float(mag[order[0]])
        top2 = float(mag[order[1]])
        if top2 <= 1e-12:
            return fallback

        # å³°å€¼ä¸å¤ çªå‡º => ä¸ç›¸ä¿¡é€±æœŸ
        if (top1 / top2) < 1.25:
            return fallback

        freq = np.fft.rfftfreq(len(xc), d=1)
        f = float(freq[order[0]])
        if f <= 1e-6:
            return fallback

        p = int(round(1.0 / f))
        return int(np.clip(p, lo, hi))

    r180 = ret.dropna().tail(180).values
    cycle_p = pick_cycle_from_fft(r180, lo=40, hi=120, fallback=80)

    v180 = df["Volume"].dropna().tail(180).astype(float).values
    cycle_v = pick_cycle_from_fft(v180, lo=20, hi=60, fallback=35)

    # -----------------------------
    # 5) éœ‡ç›ªå¹…åº¦ base_ampï¼ˆåªç”¨ä¾†ç•«ã€Œåƒå¸‚å ´ã€çš„ oscillationï¼‰
    # -----------------------------
    rsi_strength = abs(float(rsi) - 50.0) / 50.0
    rsi_factor = float(np.clip(0.6 + 0.8 * rsi_strength, 0.7, 1.25))
    if rsi > 75:
        rsi_factor *= 0.75

    base_amp = float(np.clip(atr_ratio * rsi_factor, 0.02, 0.18))
    base_amp = float(np.clip(base_amp * float(amp), 0.02, 0.22))
        # ğŸ”¥ 8110 å°ˆå±¬ï¼šRSI æ¥µåº¦éç†±æ™‚ï¼Œå†å£“ä¸€æ¬¡éœ‡ç›ªå¹…åº¦ï¼ˆé¿å…éç†±é‚„ç•«å¤§æµªå¾€ä¸Šï¼‰
    if rsi > 75:
        base_amp *= 0.75

    # -----------------------------
    # 6) baseline trendï¼ˆç´” drift è·¯å¾‘ï¼‰
    # -----------------------------
    trend = []
    p = float(last_close)
    for _ in range(MONTHS):
        p *= np.exp(monthly_logret)
        trend.append(p)
    trend = np.array(trend, dtype=float)

    # -----------------------------
    # 7) confï¼šå½±éŸ¿ã€Œä½ ä¿¡æ¨¡å‹å¤šå°‘ã€+ã€Œä¸ç¢ºå®šæ€§å¤šå¯¬ã€
    # -----------------------------
    if pred_dir_last is None:
        conf = 0.35
    else:
        try:
            pdv = float(pred_dir_last)
            conf = abs(pdv - 0.5) * 2.0  # 0~1
            conf = float(np.clip(conf, 0.0, 1.0))
        except Exception:
            conf = 0.35

    prices = [float(last_close)]
    centers = [float(last_close)]

    for m in range(1, MONTHS + 1):
        phase_p = 2 * np.pi * (m * DPM) / float(cycle_p)
        phase_v = 2 * np.pi * (m * DPM) / float(cycle_v)

        cycle_main = base_amp * np.sin(phase_p)
        cycle_pull = 0.6 * base_amp * np.sin(phase_v + np.pi)

        # è¶Šä¹…è¶Šä¸ä¿¡æ¨¡å‹ï¼›conf è¶Šä½ä¹Ÿè¶Šä¸ä¿¡
        w_time = float(np.exp(-0.55 * (m - 1)))
        w_conf = 0.25 + 0.75 * conf
        w = float(np.clip(w_time * w_conf, 0.05, 0.90))

        # centerï¼šç”¨ã€Œæœˆ1 anchorã€ä¾†æä¾› edgeï¼Œä½†ä¸è®“å®ƒä¸»å®°å¤ªä¹…
        center = w * model_1m_price + (1 - w) * float(trend[m - 1])
        price = center * (1 + cycle_main + cycle_pull)

        centers.append(float(center))
        prices.append(float(price))

    prices = np.array(prices, dtype=float)
    centers = np.array(centers, dtype=float)

    # -----------------------------
    # 8) âœ… Expected Rangeï¼šç”¨ log-return backtest errorï¼ˆæ›´è²¼è¿‘çœŸå¯¦ï¼‰
    # -----------------------------
    def load_recent_logret_errors(ticker, max_files=120):
        files = sorted(glob.glob(f"results/*_{ticker}_backtest.csv"))[-max_files:]
        errs = []
        for f in files:
            try:
                bt = pd.read_csv(f)
                close_t = float(bt["close_t"].iloc[0])
                pred_t1 = float(bt["pred_t1"].iloc[0])
                actual_t1 = float(bt["actual_t1"].iloc[0])

                if close_t <= 0 or pred_t1 <= 0 or actual_t1 <= 0:
                    continue

                # ç”¨ã€Œç›¸å°å ±é…¬ã€çš„èª¤å·®ï¼š (actual/close) - (pred/close) åœ¨ log space
                e = np.log(actual_t1 / close_t) - np.log(pred_t1 / close_t)
                e = float(e)
                if np.isfinite(e):
                    errs.append(e)
            except Exception:
                pass
        return np.array(errs, dtype=float)

    log_errs = load_recent_logret_errors(ticker)
    t = np.arange(len(prices), dtype=float)
    scale_t = np.sqrt(np.maximum(t, 1.0))  # âˆšt æ“´æ•£ï¼ˆåƒéš¨æ©Ÿæ¸¸èµ°ï¼‰

    # âœ… conf å½±éŸ¿ band å¯¬åº¦ï¼šè¶Šæ²’æŠŠæ¡è¶Šå¯¬
    # conf=1 => factor ~1.0ï¼›conf=0 => factor ~1.8
    unc_factor = float(1.0 + 0.8 * (1.0 - conf))

    if len(log_errs) >= 25:
        q10, q90 = np.quantile(log_errs, [0.10, 0.90])  # 10~90% å€é–“
        # band åœ¨ log ç©ºé–“æ“´æ•£ï¼Œæœ€å¾Œè½‰å›åƒ¹æ ¼
        upper = centers * np.exp(float(q90) * scale_t * unc_factor)
        lower = centers * np.exp(float(q10) * scale_t * unc_factor)
    else:
        # fallbackï¼šç”¨ ATR% åš log-band
        # ä»¥ atr_ratio ç•¶æ—¥æ³¢å‹•ä»£ç†ï¼Œæœˆæ“´æ•£ç´„ âˆšt
        sigma = float(np.clip(atr_ratio, 0.01, 0.20))
        k = 1.05  # å¤§æ¦‚å°æ‡‰ 10~90 çš„ç²—ç•¥å°ºåº¦
        upper = centers * np.exp(+k * sigma * scale_t * unc_factor)
        lower = centers * np.exp(-k * sigma * scale_t * unc_factor)

    # -----------------------------
    # 9) X label
    # -----------------------------
    labels = ["Now"] + [
        (asof_date + pd.DateOffset(months=i)).strftime("%Y-%m")
        for i in range(1, MONTHS + 1)
    ]

    # -----------------------------
    # 10) Plotï¼ˆæª”åä¸è®Šï¼‰
    # -----------------------------
    plt.figure(figsize=(15, 7))
    x = np.arange(MONTHS + 1)

    plt.fill_between(x, lower, upper, alpha=0.18, label="Expected Range (10-90%)")
    plt.plot(x, prices, "r-o", linewidth=2.8, label="Projected Path")
    plt.scatter(0, prices[0], s=180, marker="*", label="Today")

    for i, p in enumerate(prices[1:]):
        plt.text(i + 1, p, f"{p:.2f}", ha="center", fontsize=12)

    info = (
        f"asof={asof_date.date()} | model_1M={model_1m_price:.2f} | amp={amp:.2f} | conf={conf:.2f} | unc={unc_factor:.2f}\n"
        f"drift(d)={daily_drift_adj:.5f} | trend_score={trend_score:.2f} | ATR%={atr_ratio:.2%} | RSI={float(rsi):.2f}\n"
        f"cycle_p={cycle_p} | cycle_v={cycle_v} | base_amp={base_amp:.3f} | edge(d)={edge_daily:.4f} | vol_cap(m)={vol_cap:.3f}"
    )
    plt.gca().text(
        0.01, 0.02, info,
        transform=plt.gca().transAxes,
        fontsize=9,
        alpha=0.55,
        ha="left",
        va="bottom"
    )

    plt.xticks(x, labels, fontsize=13)
    plt.title(f"{ticker} Â· 6M Outlook (Realistic Trend + Calibrated Uncertainty)")
    plt.grid(alpha=0.3)
    plt.legend()

    os.makedirs("results", exist_ok=True)
    out = f"results/{datetime.now():%Y-%m-%d}_{ticker}_6m_advanced.png"
    plt.savefig(out, dpi=300, bbox_inches="tight")
    plt.close()


def last_valid_value(df: pd.DataFrame, col: str, lookback: int = 30):
    """
    å–æœ€è¿‘ä¸€ç­†æœ‰æ•ˆï¼ˆé NaNï¼‰çš„æŒ‡æ¨™å€¼
    - ç”¨æ–¼éäº¤æ˜“æ—¥ / è£œ today row çš„æƒ…æ³
    """
    if col not in df.columns:
        return None

    s = df[col].iloc[-lookback:]
    s = s[s.notna()]
    if s.empty:
        return None
    return float(s.iloc[-1])



# ================= Main =================
# ================= Main =================
if __name__ == "__main__":
    TICKER = "8110.TW"
    COLLECTION = "NEW_stock_data_liteon"

    # âœ… è¯æ±å°ˆå±¬è¨­å®šï¼ˆnormalized return ç‰ˆæœ¬ï¼‰
    STOCK_CONFIG = {
        "8110.TW": {
            "LOOKBACK": 40,
            "STEPS": 5,                 # 5æ—¥ï¼šreturn head ç”¨
            "MAX_DAILY_NORMRET": 3.0,   # normalized return é™å¹…ï¼ˆ2~4 å¸¸è¦‹ï¼‰
            "LR": 6e-4,
            "LSTM_UNITS": 64
        },
    }

    cfg = STOCK_CONFIG.get(TICKER, {
        "LOOKBACK": 40,
        "STEPS": 5,
        "MAX_DAILY_NORMRET": 3.0,
        "LR": 6e-4,
        "LSTM_UNITS": 64
    })

    LOOKBACK = cfg["LOOKBACK"]
    STEPS = cfg["STEPS"]

    # âœ… è¶¨å‹¢ head è¨­å®šï¼ˆæœ€æœ‰æ„Ÿï¼‰
    TREND_H = 20      # 20 äº¤æ˜“æ—¥ â‰ˆ 1 å€‹æœˆè¶¨å‹¢
    K_FLAT  = 0.8     # ç›¤æ•´é–€æª»ï¼ˆ0.6~1.2ï¼›è¶Šå¤§è¶Šä¿å®ˆï¼‰

    os.makedirs("models", exist_ok=True)
    MODEL_PATH = f"models/{TICKER}_attn_lstm.keras"

    # ---------- Data ----------
    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    df = ensure_latest_trading_row(df)
    df = add_features(df)

    # âœ… è¯æ±å°ˆå±¬ç‰¹å¾µï¼ˆå« OHLC + æ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰
    FEATURES = [
        "Close", "Open", "High", "Low",
        "Volume", "RSI", "MACD", "K", "D", "ATR_14",
        "HL_RANGE", "GAP", "VOL_REL",
        "TREND_60",
        "TREND_SLOPE_20"
    ]

    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(
            f"âš ï¸ Firestore è³‡æ–™ç¼ºæ¬„ä½ï¼š{missing}\n"
            f"è«‹ç¢ºèª catch_stock.py å¯«å› 8110.TW æ™‚åŒ…å« Open/High/Low/Close/Volumeï¼Œä¸”æŒ‡æ¨™æ¬„ä½å·²å¯«å…¥ã€‚"
        )

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    df = df.dropna()

    # âœ… create_sequencesï¼šæœƒå›å‚³ y_trend3ï¼ˆ3é¡è¶¨å‹¢ï¼‰
    X, y_ret, y_dir, y_trend3, idx = create_sequences(
        df, FEATURES,
        steps=STEPS,
        window=LOOKBACK,
        trend_h=TREND_H,
        k_flat=K_FLAT
    )
    print(f"df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 60:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<60ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK æˆ–å¢åŠ  days/æª¢æŸ¥ NaNã€‚")

    # ---------- Time-series split ----------
    split = int(len(X) * 0.85)
    X_tr, X_va = X[:split], X[split:]
    y_ret_tr, y_ret_va = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_va = y_dir[:split], y_dir[split:]
    y_tr3_tr, y_tr3_va = y_trend3[:split], y_trend3[split:]
    idx_tr, idx_va = idx[:split], idx[split:]

    # âœ… scaler.fit åƒ…ç”¨ train å€é–“ï¼ˆé¿å… leakageï¼‰
    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + max(STEPS, TREND_H) + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    sx = MinMaxScaler()
    sx.fit(df_for_scaler.values)

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_va_s = scale_X(X_va)

    # ---------- Model ----------
    if os.path.exists(MODEL_PATH):
        print(f"âœ… è¼‰å…¥æ—¢æœ‰æ¨¡å‹ï¼š{MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    else:
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_normret=cfg["MAX_DAILY_NORMRET"],
            learning_rate=cfg["LR"],
            lstm_units=cfg["LSTM_UNITS"]
        )

    # âœ… çœŸæ­£æ™‚é–“åºåˆ— validationï¼ˆæœ€å¾Œ 15%ï¼‰
    model.fit(
        X_tr_s,
        {"return": y_ret_tr, "direction": y_dir_tr, "trend3": y_tr3_tr},
        validation_data=(X_va_s, {"return": y_ret_va, "direction": y_dir_va, "trend3": y_tr3_va}),
        epochs=120,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(monitor="val_loss", patience=12, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ å·²å„²å­˜æ¨¡å‹ï¼š{MODEL_PATH}")

    # ---------- Predict (use validation tail as "latest unseen") ----------
    pred_ret, pred_dir, pred_tr3 = model.predict(X_va_s, verbose=0)

    raw_norm_returns = pred_ret[-1]         # 5æ—¥ normalized returnï¼ˆå·²é™å¹…ï¼‰
    p_dir = float(pred_dir[-1][0])          # 5æ—¥çœ‹æ¼²æ©Ÿç‡
    p_tr = pred_tr3[-1].astype(float)       # 20æ—¥è¶¨å‹¢ä¸‰é¡ [Down, Flat, Up]
    trend_label = ["Down", "Flat", "Up"][int(np.argmax(p_tr))]

    print(f"ğŸ“ˆ 5D çœ‹æ¼²æ©Ÿç‡: {p_dir:.2%}")
    print(f"ğŸ“Œ 20D è¶¨å‹¢: {trend_label} | P(Down/Flat/Up) = {p_tr[0]:.2f}/{p_tr[1]:.2f}/{p_tr[2]:.2f}")

    # ---------- Asof date ----------
    asof_date, is_today_trading = get_asof_trading_day(df)
    if not is_today_trading:
        print(f"â„¹ï¸ ä»Šæ—¥éäº¤æ˜“æ—¥ï¼Œ{TICKER} ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥ {asof_date.date()}")

    last_close = float(df.loc[asof_date, "Close"])

    # âœ… æŠŠ normalized return ä¹˜å›æ³¢å‹•å°ºåº¦ï¼ˆç”¨ asof çš„ RET_STD_20ï¼‰
    scale_last = float(df.loc[asof_date, "RET_STD_20"])
    if not np.isfinite(scale_last) or scale_last <= 0:
        scale_last = float(np.log(df["Close"].astype(float)).diff().rolling(20).std().iloc[-1])
    scale_last = max(scale_last, 1e-6)

    # ğŸ”§ Regime-based ampï¼ˆä¿ç•™çµ¦ 6M é€±æœŸéœ‡ç›ªç”¨ï¼‰
    trend60 = last_valid_value(df, "TREND_60", lookback=5)
    amp = 1.0
    if trend60 is not None:
        if trend60 > 1.0:
            amp = 1.4
        elif trend60 < -1.0:
            amp = 1.3
        elif abs(trend60) < 0.5:
            amp = 0.6

    print(f"ğŸ“Š Regime amp = {amp:.2f}")

    # ---------- 5D price projection ----------
    # âœ… æœ€æœ‰æ„Ÿä¿®æ­£ï¼š5æ—¥æ¨å›åƒ¹æ ¼ä¸è¦ä¹˜ ampï¼ˆé¿å…æ”¾å¤§å™ªéŸ³ï¼‰
    prices = []
    price = last_close
    for r_norm in raw_norm_returns:
        r = float(r_norm) * scale_last
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    future_df["date"] = pd.bdate_range(
        start=asof_date + BDay(1),
        periods=STEPS
    )

    # âœ… é æ¸¬æ•¸å€¼è¼¸å‡º CSVï¼ˆæª”åå« tickerï¼‰
    os.makedirs("results", exist_ok=True)
    forecast_csv = f"results/{datetime.now():%Y-%m-%d}_{TICKER}_forecast.csv"
    future_df.to_csv(forecast_csv, index=False, encoding="utf-8-sig")

    # âœ… åœ–è¼¸å‡ºï¼ˆå…§å®¹ä¸å‹•ã€æª”åå« tickerï¼‰
    plot_and_save(df, future_df, ticker=TICKER)
    plot_backtest_error(df, ticker=TICKER)

    # ---------- 6M Outlook (advanced) ----------
    # ç”¨æœ€å¾Œä¸€ç­†æ–¹å‘æ©Ÿç‡åš confï¼ˆä½ åŸæœ¬è¨­è¨ˆï¼‰
    pred_dir_last = float(p_dir)

    plot_6m_trend_advanced(
        df=df,
        last_close=last_close,
        raw_norm_returns=raw_norm_returns,
        scale_last=scale_last,
        ticker=TICKER,
        asof_date=asof_date,
        amp=amp,
        pred_ret_all=pred_ret,          # âœ… å¯ç”¨ ensemble
        pred_dir_last=pred_dir_last,
        k_ens=20
    )

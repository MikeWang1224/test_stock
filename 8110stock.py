# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py  (8110stock.py)
- Attention-LSTM
- Multi-task: Return path + Direction
- âœ… å°è³‡æ–™å‹å–„ç‰ˆï¼šæ›´ç©©ã€æ›´ä¸å®¹æ˜“äº‚å™´
  1) LOOKBACK=40, STEPS=5
  2) LSTM + Attention poolingï¼ˆåƒæ•¸æ¯” Transformer æ›´é©åˆå°è³‡æ–™ï¼‰
  3) âœ… Return head åŠ  tanh é™å¹…ï¼ˆé¿å…é æ¸¬çˆ†ç‚¸ï¼‰
  4) âœ… Volume åš log1pï¼ˆå°è³‡æ–™æ›´ç©©ï¼‰ 
- åœ–è¡¨è¼¸å‡ºå®Œå…¨ä¸è®Šï¼ˆä¿ç•™ Today æ¨™è¨˜ï¼‰

âœ… æ”¹1ï¼šä¿®æ­£ scaler fit / split åº§æ¨™ç³»ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼ˆleakageï¼‰
  - create_sequences å›å‚³æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æ—¥æœŸ idx
  - split ç”¨æ¨£æœ¬æ•¸åˆ‡ï¼Œscaler.fit åªç”¨ train å€é–“çš„ df ç‰¹å¾µ

âœ… æ–°å¢ï¼šåŒæ™‚è¼¸å‡º PNG + CSVï¼ˆæª”åå« tickerï¼‰
  - results/YYYY-MM-DD_TICKER_pred.png
  - results/YYYY-MM-DD_TICKER_forecast.csv
  - results/YYYY-MM-DD_TICKER_backtest.png
  - results/YYYY-MM-DD_TICKER_backtest.csv

âœ… è¯æ± 8110.TW å°ˆå±¬å¼·åŒ–ï¼ˆç…§å‰é¢å»ºè­°æ”¹ï¼‰
  A) Featureï¼šåŠ å…¥ HL_RANGE / GAP / VOL_RELï¼ˆæ›´è²¼è¿‘ä¸­å°å‹è‚¡/æ³¢å‹•è‚¡ï¼‰
  B) Targetï¼šé æ¸¬ã€Œæ³¢å‹•æ¨™æº–åŒ–ã€log-returnï¼ˆç”¨ t-1 çš„ RET_STD_20 åšå°ºåº¦ï¼Œé¿å…å·çœ‹ï¼‰
  C) æ¨å›åƒ¹æ ¼æ™‚ï¼šæŠŠé æ¸¬çš„ normalized return ä¹˜å› asof çš„ RET_STD_20
  D) loss_weightsï¼šdirection æ¬Šé‡æé«˜ï¼ˆæ–¹å‘é€šå¸¸æ¯”ç²¾æº–åƒ¹æ›´å¯é ï¼‰
"""

import os, json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout,
    Softmax, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()

# ================= Firestore è®€å– =================
def load_df_from_firestore(ticker, collection="NEW_stock_data_liteon", days=500):
    rows = []
    if db:
        for doc in db.collection(collection).stream():
            p = doc.to_dict().get(ticker)
            if p:
                rows.append({"date": doc.id, **p})

    df = pd.DataFrame(rows)
    if df.empty:
        raise ValueError("âš ï¸ Firestore ç„¡è³‡æ–™")

    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").tail(days).set_index("date")
    return df

# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_today_row(df):
    today = pd.Timestamp(datetime.now().date())
    last_date = df.index.max()
    if last_date < today:
        df.loc[today] = df.loc[last_date]
        print(f"âš ï¸ ä»Šæ—¥ç„¡è³‡æ–™ï¼Œä½¿ç”¨ {last_date.date()} è£œä»Šæ—¥")
    return df.sort_index()

# ================= Feature Engineeringï¼ˆè¯æ±å°ˆå±¬ï¼‰ =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # âœ… Volume å°ºåº¦ç©©å®šï¼ˆéå¸¸å»ºè­°ï¼‰
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    # åœ–è¡¨ç”¨å‡ç·šï¼ˆä¿æŒä¸è®Šï¼‰
    df["SMA5"] = df["Close"].rolling(5).mean()
    df["SMA10"] = df["Close"].rolling(10).mean()

    # âœ… è¯æ±ï¼ˆæ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰ç‰¹å¾µ
    # éœ€è¦ Firestore æœ‰ Open/High/Lowï¼ˆä½  catch_stock.py å¯«å…¥æ˜¯æœ‰çš„ï¼‰
    if all(c in df.columns for c in ["Open", "High", "Low", "Close"]):
        df["HL_RANGE"] = (df["High"].astype(float) - df["Low"].astype(float)) / df["Close"].astype(float)
        df["GAP"] = (df["Open"].astype(float) - df["Close"].shift(1).astype(float)) / df["Close"].shift(1).astype(float)
    else:
        # è‹¥ç¼ºæ¬„ä½ï¼Œå…ˆçµ¦ NaNï¼ˆå¾Œé¢ dropna æœƒæ’æ‰ï¼‰
        df["HL_RANGE"] = np.nan
        df["GAP"] = np.nan

    # âœ… é‡èƒ½ç›¸å°å¼·å¼±ï¼ˆç”¨ log1p ä¹‹å¾Œçš„ Volume å»åšæ¯”å€¼å³å¯ï¼‰
    df["VOL_REL"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)

    # âœ… 20æ—¥æ³¢å‹•ï¼ˆç”¨ä¾†æ¨™æº–åŒ– yï¼‰
    close = df["Close"].astype(float)
    df["RET_STD_20"] = np.log(close).diff().rolling(20).std()

    return df

# ================= Sequenceï¼ˆæ¨™æº–åŒ– returnï¼Œé¿å…æ³¢å‹• regime å½±éŸ¿ï¼‰ =================
def create_sequences(df, features, steps=5, window=40, eps=1e-9):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 çš„ã€Œnormalized log returnã€
           normalized = logret / (RET_STD_20 at t-1)
           âœ… ç”¨ t-1 çš„æ³¢å‹•ç•¶å°ºåº¦ï¼Œé¿å…å·çœ‹ï¼ˆno leakageï¼‰
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆç”¨ raw logret åˆ¤æ–·ï¼‰
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰çš„ã€Œt ç•¶å¤©æ—¥æœŸã€
    """
    X, y_ret, y_dir, idx = [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    feat = df[features].values

    for i in range(window, len(df) - steps):
        x_seq = feat[i - window:i]

        future_ret_raw = logret.iloc[i:i + steps].values
        if np.any(np.isnan(future_ret_raw)) or np.any(np.isnan(x_seq)):
            continue

        # âœ… ç”¨ t-1 çš„æ³¢å‹•å°ºåº¦ï¼ˆé¿å…å·çœ‹ t çš„è³‡è¨Šï¼‰
        scale = df["RET_STD_20"].iloc[i - 1]
        if pd.isna(scale) or scale < eps:
            continue

        future_ret_norm = future_ret_raw / (float(scale) + eps)

        X.append(x_seq)
        y_ret.append(future_ret_norm)
        y_dir.append(1.0 if future_ret_raw.sum() > 0 else 0.0)
        idx.append(df.index[i])

    return np.array(X), np.array(y_ret), np.array(y_dir), np.array(idx)

# ================= Attention-LSTMï¼ˆâœ… return é™å¹… + direction æ¬Šé‡æé«˜ï¼‰ =================
def build_attention_lstm(input_shape, steps, max_daily_normret=3.0, learning_rate=6e-4, lstm_units=64):
    """
    max_daily_normretï¼šé™åˆ¶ã€Œnormalizedã€å–®æ—¥å¹…åº¦ï¼Œé¿å…é€£ä¹˜çˆ†ç‚¸
    - å› ç‚ºç¾åœ¨ y æ˜¯ normalized returnï¼Œæ‰€ä»¥é™å¹…æ‡‰è©²ç”¨ã€Œå€æ•¸ã€è€Œä¸æ˜¯ 0.06 é€™ç¨®çµ•å°å€¼
    å¸¸è¦‹åˆç†ç¯„åœï¼š2 ~ 4
    """
    inp = Input(shape=input_shape)

    x = LSTM(lstm_units, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    # âœ… return headï¼štanh é™å¹…ï¼ˆnormalized returnï¼‰
    raw = Dense(steps, activation="tanh")(context)           # [-1, 1]
    out_ret = Lambda(lambda t: t * max_daily_normret, name="return")(raw)

    out_dir = Dense(1, activation="sigmoid", name="direction")(context)

    model = Model(inp, [out_ret, out_dir])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": "binary_crossentropy"
        },
        # âœ… è¯æ±ï¼šæ–¹å‘æ›´é‡è¦ä¸€äº›
        loss_weights={
            "return": 1.0,
            "direction": 0.8
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆå…§å®¹ä¸å‹•ï¼šæ–°å¢ Today æ¨™è¨˜ï¼‰ =================
def plot_and_save(df_hist, future_df, ticker: str):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18,8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    # âœ… Today é»èˆ‡æ–‡å­—ï¼ˆhist æœ€å¾Œä¸€å€‹é»ï¼‰
    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}",
            fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}",
                color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title("2301.TW Attention-LSTM é æ¸¬")  # âœ… å…§å®¹ä¸å‹•

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
def plot_backtest_error(df, ticker: str):
    """
    æ±ºç­–å¼å›æ¸¬åœ–ï¼ˆDecision-based Backtestï¼‰
    - è‡ªå‹•æ’é™¤ä»Šå¤©çš„ forecast
    - ä½¿ç”¨æœ€è¿‘ä¸€ç­†æ­·å² forecastï¼ˆåŒ tickerï¼‰
    """

    today = pd.Timestamp(datetime.now().date())

    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    forecast_files = []
    for f in os.listdir("results"):
        if not f.endswith(f"_{ticker}_forecast.csv"):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
        except Exception:
            continue
        if d < today:
            forecast_files.append((d, f))

    if not forecast_files:
        print("âš ï¸ æ‰¾ä¸åˆ°å¯ç”¨çš„æ­·å² forecastï¼ˆå·²æ’é™¤ä»Šå¤© & å·²é™å®š tickerï¼‰")
        return

    forecast_files.sort(key=lambda x: x[0], reverse=True)
    forecast_date, forecast_name = forecast_files[0]
    forecast_csv = os.path.join("results", forecast_name)

    print(f"ğŸ“„ Backtest ä½¿ç”¨ forecastï¼š{forecast_name}")

    future_df = pd.read_csv(forecast_csv, parse_dates=["date"])

    valid_days = df.index[df.index < today]
    if len(valid_days) < 2:
        print("âš ï¸ ç„¡è¶³å¤ æ­·å²äº¤æ˜“æ—¥ï¼Œç•¥éå›æ¸¬")
        return

    t = valid_days[-1]
    t1 = t + BDay(1)

    close_t = float(df.loc[t, "Close"])
    pred_t1 = float(future_df.loc[0, "Pred_Close"])

    if t1 in df.index:
        actual_t1 = float(df.loc[t1, "Close"])
    else:
        actual_t1 = float(df["Close"].iloc[-1])

    trend = df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1], "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1], "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    dx = 0.08
    price_offset = max(0.2, close_t * 0.002)

    ax.text(x_t, close_t + price_offset, f"{close_t:.2f}", ha="center", va="bottom", fontsize=18, color="black")
    ax.text(x_t + 1 + dx, pred_t1, f"Pred {pred_t1:.2f}", ha="left", va="center", fontsize=16, color="red")
    ax.text(x_t + 1 + dx, actual_t1, f"Actual {actual_t1:.2f}", ha="left", va="center", fontsize=16, color="green")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title("2301.TW Decision Backtest (t â†’ t+1)")  # âœ… å…§å®¹ä¸å‹•
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8,
        alpha=0.4,
        ha="left",
        va="bottom"
    )

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{today:%Y-%m-%d}_{ticker}_backtest.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    out_csv = f"results/{today:%Y-%m-%d}_{ticker}_backtest.csv"
    bt.to_csv(out_csv, index=False, encoding="utf-8-sig")

# ================= Main =================
if __name__ == "__main__":
    TICKER = "8110.TW"
    COLLECTION = "NEW_stock_data_liteon"

    # âœ… è¯æ±å°ˆå±¬è¨­å®šï¼ˆnormalized return ç‰ˆæœ¬ï¼‰
    STOCK_CONFIG = {
        "8110.TW": {
            "LOOKBACK": 40,
            "STEPS": 5,
            "MAX_DAILY_NORMRET": 3.0,  # normalized return é™å¹…ï¼ˆ2~4 å¸¸è¦‹ï¼‰
            "LR": 6e-4,
            "LSTM_UNITS": 64
        },
    }

    cfg = STOCK_CONFIG.get(TICKER, {
        "LOOKBACK": 40,
        "STEPS": 5,
        "MAX_DAILY_NORMRET": 3.0,
        "LR": 6e-4,
        "LSTM_UNITS": 64
    })

    LOOKBACK = cfg["LOOKBACK"]
    STEPS = cfg["STEPS"]

    os.makedirs("models", exist_ok=True)
    MODEL_PATH = f"models/{TICKER}_attn_lstm.keras"

    # ---------- Data ----------
    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    df = ensure_today_row(df)
    df = add_features(df)

    # âœ… è¯æ±å°ˆå±¬ç‰¹å¾µï¼ˆå« OHLC + æ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰
    FEATURES = [
        "Close", "Open", "High", "Low",
        "Volume", "RSI", "MACD", "K", "D", "ATR_14",
        "HL_RANGE", "GAP", "VOL_REL"
    ]

    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(
            f"âš ï¸ Firestore è³‡æ–™ç¼ºæ¬„ä½ï¼š{missing}\n"
            f"è«‹ç¢ºèª catch_stock.py å¯«å› 8110.TW æ™‚åŒ…å« Open/High/Low/Close/Volumeï¼Œä¸”æŒ‡æ¨™æ¬„ä½å·²å¯«å…¥ã€‚"
        )

    # RET_STD_20 æ˜¯ y çš„å°ºåº¦ï¼Œéœ€è¦ä¸€èµ·å­˜åœ¨ï¼ˆadd_features æœƒåšï¼‰
    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    df = df.dropna()

    X, y_ret, y_dir, idx = create_sequences(df, FEATURES, steps=STEPS, window=LOOKBACK)
    print(f"df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 40:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<40ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK/STEPS æˆ–æª¢æŸ¥è³‡æ–™æ˜¯å¦ç¼ºæ¬„ä½/éå¤š NaNã€‚")

    split = int(len(X) * 0.85)

    X_tr, X_te = X[:split], X[split:]
    y_ret_tr, y_ret_te = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_te = y_dir[:split], y_dir[split:]
    idx_tr, idx_te = idx[:split], idx[split:]

    # âœ… scaler.fit åƒ…ç”¨ train å€é–“ï¼ˆç”¨ idx_tr çš„æœ€å¾Œæ—¥æœŸç•Œå®šï¼‰
    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    sx = MinMaxScaler()
    sx.fit(df_for_scaler.values)

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_te_s = scale_X(X_te)

    # ---------- Model (å°ˆå±¬) ----------
    if os.path.exists(MODEL_PATH):
        print(f"âœ… è¼‰å…¥æ—¢æœ‰æ¨¡å‹ï¼š{MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    else:
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_normret=cfg["MAX_DAILY_NORMRET"],
            learning_rate=cfg["LR"],
            lstm_units=cfg["LSTM_UNITS"]
        )

    model.fit(
        X_tr_s,
        {"return": y_ret_tr, "direction": y_dir_tr},
        epochs=80,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ å·²å„²å­˜æ¨¡å‹ï¼š{MODEL_PATH}")

    pred_ret, pred_dir = model.predict(X_te_s, verbose=0)
    raw_norm_returns = pred_ret[-1]  # âœ… normalized returnsï¼ˆå·²é™å¹…ï¼‰

    print(f"ğŸ“ˆ é æ¸¬æ–¹å‘æ©Ÿç‡ï¼ˆçœ‹æ¼²ï¼‰: {pred_dir[-1][0]:.2%}")

    asof_date = df.index.max()
    last_close = float(df.loc[asof_date, "Close"])

    # âœ… æŠŠ normalized return ä¹˜å›æ³¢å‹•å°ºåº¦ï¼ˆç”¨ asof çš„ RET_STD_20ï¼‰
    scale_last = float(df.loc[asof_date, "RET_STD_20"])
    if not np.isfinite(scale_last) or scale_last <= 0:
        # fallbackï¼šç”¨æœ€è¿‘ 20 å¤© std ä¼°
        scale_last = float(np.log(df["Close"].astype(float)).diff().rolling(20).std().iloc[-1])
    scale_last = max(scale_last, 1e-6)

    prices = []
    price = last_close
    for r_norm in raw_norm_returns:
        r = float(r_norm) * scale_last
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    future_df["date"] = pd.bdate_range(
        start=df.index.max() + BDay(1),
        periods=STEPS
    )

    # âœ… é æ¸¬æ•¸å€¼è¼¸å‡º CSVï¼ˆæª”åå« tickerï¼‰
    os.makedirs("results", exist_ok=True)
    forecast_csv = f"results/{datetime.now():%Y-%m-%d}_{TICKER}_forecast.csv"
    future_df.to_csv(forecast_csv, index=False, encoding="utf-8-sig")

    # âœ… åœ–è¼¸å‡ºï¼ˆå…§å®¹ä¸å‹•ã€æª”åæ”¹å« tickerï¼‰
    plot_and_save(df, future_df, ticker=TICKER)
    plot_backtest_error(df, ticker=TICKER)

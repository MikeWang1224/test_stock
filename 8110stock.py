#8110stock 


# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py  (8110stock.py)
- Attention-LSTM
- Multi-task: Return path + Direction
- âœ… å°è³‡æ–™å‹å–„ç‰ˆï¼šæ›´ç©©ã€æ›´ä¸å®¹æ˜“äº‚å™´
  1) LOOKBACK=40, STEPS=5
  2) LSTM + Attention poolingï¼ˆåƒæ•¸æ¯” Transformer æ›´é©åˆå°è³‡æ–™ï¼‰
  3) âœ… Return head åŠ  tanh é™å¹…ï¼ˆé¿å…é æ¸¬çˆ†ç‚¸ï¼‰
  4) âœ… Volume åš log1pï¼ˆå°è³‡æ–™æ›´ç©©ï¼‰ 
- åœ–è¡¨è¼¸å‡ºå®Œå…¨ä¸è®Šï¼ˆä¿ç•™ Today æ¨™è¨˜ï¼‰

âœ… æ”¹1ï¼šä¿®æ­£ scaler fit / split åº§æ¨™ç³»ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼ˆleakageï¼‰
  - create_sequences å›å‚³æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æ—¥æœŸ idx
  - split ç”¨æ¨£æœ¬æ•¸åˆ‡ï¼Œscaler.fit åªç”¨ train å€é–“çš„ df ç‰¹å¾µ

âœ… æ–°å¢ï¼šåŒæ™‚è¼¸å‡º PNG + CSVï¼ˆæª”åå« tickerï¼‰
  - results/YYYY-MM-DD_TICKER_pred.png
  - results/YYYY-MM-DD_TICKER_forecast.csv
  - results/YYYY-MM-DD_TICKER_backtest.png
  - results/YYYY-MM-DD_TICKER_backtest.csv

âœ… è¯æ± 8110.TW å°ˆå±¬å¼·åŒ–ï¼ˆç…§å‰é¢å»ºè­°æ”¹ï¼‰
  A) Featureï¼šåŠ å…¥ HL_RANGE / GAP / VOL_RELï¼ˆæ›´è²¼è¿‘ä¸­å°å‹è‚¡/æ³¢å‹•è‚¡ï¼‰
  B) Targetï¼šé æ¸¬ã€Œæ³¢å‹•æ¨™æº–åŒ–ã€log-returnï¼ˆç”¨ t-1 çš„ RET_STD_20 åšå°ºåº¦ï¼Œé¿å…å·çœ‹ï¼‰
  C) æ¨å›åƒ¹æ ¼æ™‚ï¼šæŠŠé æ¸¬çš„ normalized return ä¹˜å› asof çš„ RET_STD_20
  D) loss_weightsï¼šdirection æ¬Šé‡æé«˜ï¼ˆæ–¹å‘é€šå¸¸æ¯”ç²¾æº–åƒ¹æ›´å¯é ï¼‰
"""

import os, json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout,
    Softmax, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()

# ================= Firestore è®€å– =================
def load_df_from_firestore(ticker, collection="NEW_stock_data_liteon", days=500):
    rows = []
    if db:
        for doc in db.collection(collection).stream():
            p = doc.to_dict().get(ticker)
            if p:
                rows.append({"date": doc.id, **p})

    df = pd.DataFrame(rows)
    if df.empty:
        raise ValueError("âš ï¸ Firestore ç„¡è³‡æ–™")

    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").tail(days).set_index("date")
    return df

# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_today_row(df):
    today = pd.Timestamp(datetime.now().date())
    last_date = df.index.max()
    if last_date < today:
        df.loc[today] = df.loc[last_date]
        print(f"âš ï¸ ä»Šæ—¥ç„¡è³‡æ–™ï¼Œä½¿ç”¨ {last_date.date()} è£œä»Šæ—¥")
    return df.sort_index()

# ================= Feature Engineeringï¼ˆè¯æ±å°ˆå±¬ï¼‰ =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # âœ… Volume å°ºåº¦ç©©å®šï¼ˆéå¸¸å»ºè­°ï¼‰
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    # åœ–è¡¨ç”¨å‡ç·šï¼ˆä¿æŒä¸è®Šï¼‰
    df["SMA5"] = df["Close"].rolling(5).mean()
    df["SMA10"] = df["Close"].rolling(10).mean()

    # âœ… è¯æ±ï¼ˆæ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰ç‰¹å¾µ
    # éœ€è¦ Firestore æœ‰ Open/High/Lowï¼ˆä½  catch_stock.py å¯«å…¥æ˜¯æœ‰çš„ï¼‰
    if all(c in df.columns for c in ["Open", "High", "Low", "Close"]):
        df["HL_RANGE"] = (df["High"].astype(float) - df["Low"].astype(float)) / df["Close"].astype(float)
        df["GAP"] = (df["Open"].astype(float) - df["Close"].shift(1).astype(float)) / df["Close"].shift(1).astype(float)
    else:
        # è‹¥ç¼ºæ¬„ä½ï¼Œå…ˆçµ¦ NaNï¼ˆå¾Œé¢ dropna æœƒæ’æ‰ï¼‰
        df["HL_RANGE"] = np.nan
        df["GAP"] = np.nan

    # âœ… é‡èƒ½ç›¸å°å¼·å¼±ï¼ˆç”¨ log1p ä¹‹å¾Œçš„ Volume å»åšæ¯”å€¼å³å¯ï¼‰
    df["VOL_REL"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)

    # âœ… 20æ—¥æ³¢å‹•ï¼ˆç”¨ä¾†æ¨™æº–åŒ– yï¼‰
    close = df["Close"].astype(float)
    df["RET_STD_20"] = np.log(close).diff().rolling(20).std()

    # ğŸ”§ ADD: Regime / æ³¢æ®µç‹€æ…‹ç‰¹å¾µï¼ˆä¸å­˜ Firebaseï¼‰
    ma60 = df["Close"].rolling(60)
    df["TREND_60"] = (df["Close"] - ma60.mean()) / (ma60.std() + 1e-9)
    
    df["TREND_SLOPE_20"] = (
        df["Close"].rolling(20).mean().diff()
    ) / df["Close"]
    
    return df


# ================= Sequenceï¼ˆæ¨™æº–åŒ– returnï¼Œé¿å…æ³¢å‹• regime å½±éŸ¿ï¼‰ =================
def create_sequences(df, features, steps=5, window=40, eps=1e-9):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 çš„ã€Œnormalized log returnã€
           normalized = logret / (RET_STD_20 at t-1)
           âœ… ç”¨ t-1 çš„æ³¢å‹•ç•¶å°ºåº¦ï¼Œé¿å…å·çœ‹ï¼ˆno leakageï¼‰
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆç”¨ raw logret åˆ¤æ–·ï¼‰
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰çš„ã€Œt ç•¶å¤©æ—¥æœŸã€
    """
    X, y_ret, y_dir, idx = [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    feat = df[features].values

    for i in range(window, len(df) - steps):
        x_seq = feat[i - window:i]

        future_ret_raw = logret.iloc[i:i + steps].values
        if np.any(np.isnan(future_ret_raw)) or np.any(np.isnan(x_seq)):
            continue

        # âœ… ç”¨ t-1 çš„æ³¢å‹•å°ºåº¦ï¼ˆé¿å…å·çœ‹ t çš„è³‡è¨Šï¼‰
        scale = df["RET_STD_20"].iloc[i - 1]
        if pd.isna(scale) or scale < eps:
            continue

        future_ret_norm = future_ret_raw / (float(scale) + eps)

        X.append(x_seq)
        y_ret.append(future_ret_norm)
        y_dir.append(1.0 if future_ret_raw.sum() > 0 else 0.0)
        idx.append(df.index[i])

    return np.array(X), np.array(y_ret), np.array(y_dir), np.array(idx)

# ================= Attention-LSTMï¼ˆâœ… return é™å¹… + direction æ¬Šé‡æé«˜ï¼‰ =================
def build_attention_lstm(input_shape, steps, max_daily_normret=3.0, learning_rate=6e-4, lstm_units=64):
    """
    max_daily_normretï¼šé™åˆ¶ã€Œnormalizedã€å–®æ—¥å¹…åº¦ï¼Œé¿å…é€£ä¹˜çˆ†ç‚¸
    - å› ç‚ºç¾åœ¨ y æ˜¯ normalized returnï¼Œæ‰€ä»¥é™å¹…æ‡‰è©²ç”¨ã€Œå€æ•¸ã€è€Œä¸æ˜¯ 0.06 é€™ç¨®çµ•å°å€¼
    å¸¸è¦‹åˆç†ç¯„åœï¼š2 ~ 4
    """
    inp = Input(shape=input_shape)

    x = LSTM(lstm_units, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    # âœ… return headï¼štanh é™å¹…ï¼ˆnormalized returnï¼‰
    raw = Dense(steps, activation="tanh")(context)           # [-1, 1]
    out_ret = Lambda(lambda t: t * max_daily_normret, name="return")(raw)

    out_dir = Dense(1, activation="sigmoid", name="direction")(context)

    model = Model(inp, [out_ret, out_dir])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": "binary_crossentropy"
        },
        # âœ… è¯æ±ï¼šæ–¹å‘æ›´é‡è¦ä¸€äº›
        loss_weights={
            "return": 1.0,
            "direction": 0.8
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆå…§å®¹ä¸å‹•ï¼šæ–°å¢ Today æ¨™è¨˜ï¼‰ =================
def plot_and_save(df_hist, future_df, ticker: str):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18,8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    # âœ… Today é»èˆ‡æ–‡å­—ï¼ˆhist æœ€å¾Œä¸€å€‹é»ï¼‰
    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}",
            fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}",
                color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title("2301.TW Attention-LSTM é æ¸¬")  # âœ… å…§å®¹ä¸å‹•

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
def plot_backtest_error(df, ticker: str):
    """
    æ±ºç­–å¼å›æ¸¬åœ–ï¼ˆDecision-based Backtestï¼‰
    - è‡ªå‹•æ’é™¤ä»Šå¤©çš„ forecast
    - ä½¿ç”¨æœ€è¿‘ä¸€ç­†æ­·å² forecastï¼ˆåŒ tickerï¼‰
    """

    today = pd.Timestamp(datetime.now().date())

    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    forecast_files = []
    for f in os.listdir("results"):
        if not f.endswith(f"_{ticker}_forecast.csv"):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
        except Exception:
            continue
        if d < today:
            forecast_files.append((d, f))

    if not forecast_files:
        print("âš ï¸ æ‰¾ä¸åˆ°å¯ç”¨çš„æ­·å² forecastï¼ˆå·²æ’é™¤ä»Šå¤© & å·²é™å®š tickerï¼‰")
        return

    forecast_files.sort(key=lambda x: x[0], reverse=True)
    forecast_date, forecast_name = forecast_files[0]
    forecast_csv = os.path.join("results", forecast_name)

    print(f"ğŸ“„ Backtest ä½¿ç”¨ forecastï¼š{forecast_name}")

    future_df = pd.read_csv(forecast_csv, parse_dates=["date"])

    valid_days = df.index[df.index < today]
    if len(valid_days) < 2:
        print("âš ï¸ ç„¡è¶³å¤ æ­·å²äº¤æ˜“æ—¥ï¼Œç•¥éå›æ¸¬")
        return

    t = valid_days[-1]
    t1 = t + BDay(1)

    close_t = float(df.loc[t, "Close"])
    pred_t1 = float(future_df.loc[0, "Pred_Close"])

    if t1 in df.index:
        actual_t1 = float(df.loc[t1, "Close"])
    else:
        actual_t1 = float(df["Close"].iloc[-1])

    trend = df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1], "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1], "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    dx = 0.08
    price_offset = max(0.2, close_t * 0.002)

    ax.text(x_t, close_t + price_offset, f"{close_t:.2f}", ha="center", va="bottom", fontsize=18, color="black")
    ax.text(x_t + 1 + dx, pred_t1, f"Pred {pred_t1:.2f}", ha="left", va="center", fontsize=16, color="red")
    ax.text(x_t + 1 + dx, actual_t1, f"Actual {actual_t1:.2f}", ha="left", va="center", fontsize=16, color="green")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title("2301.TW Decision Backtest (t â†’ t+1)")  # âœ… å…§å®¹ä¸å‹•
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8,
        alpha=0.4,
        ha="left",
        va="bottom"
    )

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{today:%Y-%m-%d}_{ticker}_backtest.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    out_csv = f"results/{today:%Y-%m-%d}_{ticker}_backtest.csv"
    bt.to_csv(out_csv, index=False, encoding="utf-8-sig")

# ================= 6M Trend Plotï¼ˆx è»¸ = æœˆï¼‰ =================
def plot_6m_trend_advanced(
    df: pd.DataFrame,
    last_close: float,
    raw_norm_returns: np.ndarray,
    scale_last: float,
    ticker: str,
    asof_date: pd.Timestamp
):
    MONTHS = 6
    DPM = 21

    # =============================
    # 1ï¸âƒ£ ä¸»å‡è¶¨å‹¢ï¼ˆæ¨¡å‹ï¼‰
    # =============================
    # =============================
# 1ï¸âƒ£ ä¸»å‡è¶¨å‹¢ï¼ˆä½é »ï¼Œä¾†è‡ªæ­·å²åƒ¹æ ¼ï¼‰
# =============================
# ç”¨è¿‘ 120 å€‹äº¤æ˜“æ—¥ä¼°è¨ˆã€Œé•·æœŸ driftã€
    log_price = np.log(df["Close"].astype(float))
    ret_ewm = log_price.diff().ewm(span=60).mean()
    
    daily_drift = float(ret_ewm.iloc[-1])
    daily_drift = np.clip(daily_drift, -0.01, 0.01)  # é˜²çˆ†ï¼ˆÂ±1% / dayï¼‰


    # ===== Regime åˆ¤æ–·ï¼ˆPriority 1ï¼‰=====
    atr = last_valid_value(df, "ATR_14", lookback=40)
    rsi = last_valid_value(df, "RSI", lookback=40)
    
    # æ³¢å‹•å¼·åº¦ï¼ˆç›¸å°åƒ¹æ ¼ï¼‰
    vol_regime = atr / last_close if atr else 0.03
    
    # è¶¨å‹¢å¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0~1ï¼‰
    trend_score = 1.0
    
    # 1ï¸âƒ£ é«˜æª”éç†± â†’ drift ä¸å¯ä¿¡
    if rsi and rsi > 75:
        trend_score *= 0.3
    elif rsi and rsi > 65:
        trend_score *= 0.6
    
    # 2ï¸âƒ£ è¶…ä½æ³¢å‹• â†’ åç›¤æ•´
    if vol_regime < 0.015:
        trend_score *= 0.5
    
    # 3ï¸âƒ£ è¶…é«˜æ³¢å‹• â†’ regime ä¸ç©©
    if vol_regime > 0.08:
        trend_score *= 0.7
    
    # æœ€çµ‚èª¿æ•´ drift
    daily_drift *= trend_score

      
    monthly_logret = daily_drift * DPM
    
    trend = []
    p = last_close
    for _ in range(MONTHS):
        p *= np.exp(monthly_logret)
        trend.append(p)
    
    trend = np.array(trend)
    


    # =============================
    # 2ï¸âƒ£ ä¸»é€±æœŸï¼ˆåƒ¹æ ¼ï¼‰
    # =============================
    close = df["Close"].iloc[-180:].values
    close = close - close.mean()

    fft_p = np.fft.rfft(close)
    freq_p = np.fft.rfftfreq(len(close), d=1)
    idx_p = np.argmax(np.abs(fft_p[1:])) + 1
    cycle_p = np.clip(int(round(1 / freq_p[idx_p])), 40, 120)

    # =============================
# 3ï¸âƒ£ å›æª”é€±æœŸï¼ˆæˆäº¤é‡ï¼‰
# =============================
    vol_series = df["Volume"].iloc[-180:].dropna().values
    
    if len(vol_series) < 60:
        cycle_v = 30  # fallback
    else:
        vol_centered = vol_series - vol_series.mean()
    
        fft_v = np.fft.rfft(vol_centered)
        freq_v = np.fft.rfftfreq(len(vol_centered), d=1)
        idx_v = np.argmax(np.abs(fft_v[1:])) + 1
        cycle_v = np.clip(int(round(1 / freq_v[idx_v])), 20, 60)


    # =============================
    # 4ï¸âƒ£ éœ‡ç›ªå¹…åº¦ï¼ˆATR Ã— RSIï¼‰
    # =============================
    atr = last_valid_value(df, "ATR_14", lookback=40)
    if atr is None:
        raise ValueError("âŒ ç„¡å¯ç”¨ ATR_14ï¼ˆæœ€è¿‘ 40 æ—¥çš†ç‚º NaNï¼‰")
    atr_ratio = atr / last_close

    rsi = last_valid_value(df, "RSI", lookback=40)
    rsi_factor = np.clip(abs(rsi - 50) / 50, 0.3, 1.2)

    base_amp = atr_ratio * rsi_factor
    base_amp = np.clip(base_amp, 0.02, 0.18)

    # =============================
    # 5ï¸âƒ£ åˆæˆåƒ¹æ ¼ï¼ˆå¤šé€±æœŸï¼‰
    # =============================
    prices = [last_close]

    for m in range(1, MONTHS + 1):
        phase_p = 2 * np.pi * (m * DPM) / cycle_p
        phase_v = 2 * np.pi * (m * DPM) / cycle_v

        cycle_main = base_amp * np.sin(phase_p)
        cycle_pull = 0.6 * base_amp * np.sin(phase_v + np.pi)

        price = trend[m - 1] * (1 + cycle_main + cycle_pull)
        prices.append(price)

    prices = np.array(prices)

    # =============================
    # 6ï¸âƒ£ å€é–“å¸¶ï¼ˆATR-based fanï¼‰
    # =============================
    time_scale = np.linspace(0.6, 1.3, len(prices))
    upper = prices * (1 + base_amp * time_scale)
    lower = prices * (1 - base_amp * time_scale)


    # =============================
    # 7ï¸âƒ£ X è»¸ï¼ˆæœˆï¼‰
    # =============================
    labels = ["Now"] + pd.date_range(
        asof_date + pd.offsets.MonthBegin(1),
        periods=MONTHS,
        freq="MS"
    ).strftime("%Y-%m").tolist()

    # =============================
    # 8ï¸âƒ£ Plot
    # =============================
    plt.figure(figsize=(15, 7))
    x = np.arange(MONTHS + 1)

    plt.fill_between(x, lower, upper, alpha=0.18, label="Expected Range")
    plt.plot(x, prices, "r-o", linewidth=2.8, label="Projected Path")
    plt.scatter(0, prices[0], s=180, marker="*", label="Today")

    for i, p in enumerate(prices[1:]):
        plt.text(i + 1, p, f"{p:.2f}", ha="center", fontsize=12)

    plt.xticks(x, labels, fontsize=13)
    plt.title(f"{ticker} Â· 6M Outlook (Multi-Cycle + ATR + RSI)")
    plt.grid(alpha=0.3)
    plt.legend()

    os.makedirs("results", exist_ok=True)
    out = f"results/{datetime.now():%Y-%m-%d}_{ticker}_6m_advanced.png"
    plt.savefig(out, dpi=300, bbox_inches="tight")
    plt.close()

def last_valid_value(df: pd.DataFrame, col: str, lookback: int = 30):
    """
    å–æœ€è¿‘ä¸€ç­†æœ‰æ•ˆï¼ˆé NaNï¼‰çš„æŒ‡æ¨™å€¼
    - ç”¨æ–¼éäº¤æ˜“æ—¥ / è£œ today row çš„æƒ…æ³
    """
    if col not in df.columns:
        return None

    s = df[col].iloc[-lookback:]
    s = s[s.notna()]
    if s.empty:
        return None
    return float(s.iloc[-1])



# ================= Main =================
if __name__ == "__main__":
    TICKER = "8110.TW"
    COLLECTION = "NEW_stock_data_liteon"

    # âœ… è¯æ±å°ˆå±¬è¨­å®šï¼ˆnormalized return ç‰ˆæœ¬ï¼‰
    STOCK_CONFIG = {
        "8110.TW": {
            "LOOKBACK": 40,
            "STEPS": 5,
            "MAX_DAILY_NORMRET": 3.0,  # normalized return é™å¹…ï¼ˆ2~4 å¸¸è¦‹ï¼‰
            "LR": 6e-4,
            "LSTM_UNITS": 64
        },
    }

    cfg = STOCK_CONFIG.get(TICKER, {
        "LOOKBACK": 40,
        "STEPS": 5,
        "MAX_DAILY_NORMRET": 3.0,
        "LR": 6e-4,
        "LSTM_UNITS": 64
    })

    LOOKBACK = cfg["LOOKBACK"]
    STEPS = cfg["STEPS"]

    os.makedirs("models", exist_ok=True)
    MODEL_PATH = f"models/{TICKER}_attn_lstm.keras"

    # ---------- Data ----------
    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    df = ensure_today_row(df)
    df = add_features(df)

    # âœ… è¯æ±å°ˆå±¬ç‰¹å¾µï¼ˆå« OHLC + æ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰
    FEATURES = [
        "Close", "Open", "High", "Low",
        "Volume", "RSI", "MACD", "K", "D", "ATR_14",
        "HL_RANGE", "GAP", "VOL_REL",
        "TREND_60",          # ğŸ”§ ADD
        "TREND_SLOPE_20"     # ğŸ”§ ADD
    ]


    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(
            f"âš ï¸ Firestore è³‡æ–™ç¼ºæ¬„ä½ï¼š{missing}\n"
            f"è«‹ç¢ºèª catch_stock.py å¯«å› 8110.TW æ™‚åŒ…å« Open/High/Low/Close/Volumeï¼Œä¸”æŒ‡æ¨™æ¬„ä½å·²å¯«å…¥ã€‚"
        )

    # RET_STD_20 æ˜¯ y çš„å°ºåº¦ï¼Œéœ€è¦ä¸€èµ·å­˜åœ¨ï¼ˆadd_features æœƒåšï¼‰
    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    df = df.dropna()

    X, y_ret, y_dir, idx = create_sequences(df, FEATURES, steps=STEPS, window=LOOKBACK)
    print(f"df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 40:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<40ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK/STEPS æˆ–æª¢æŸ¥è³‡æ–™æ˜¯å¦ç¼ºæ¬„ä½/éå¤š NaNã€‚")

    split = int(len(X) * 0.85)

    X_tr, X_te = X[:split], X[split:]
    y_ret_tr, y_ret_te = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_te = y_dir[:split], y_dir[split:]
    idx_tr, idx_te = idx[:split], idx[split:]

    # âœ… scaler.fit åƒ…ç”¨ train å€é–“ï¼ˆç”¨ idx_tr çš„æœ€å¾Œæ—¥æœŸç•Œå®šï¼‰
    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    sx = MinMaxScaler()
    sx.fit(df_for_scaler.values)

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_te_s = scale_X(X_te)

    # ---------- Model (å°ˆå±¬) ----------
    if os.path.exists(MODEL_PATH):
        print(f"âœ… è¼‰å…¥æ—¢æœ‰æ¨¡å‹ï¼š{MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    else:
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_normret=cfg["MAX_DAILY_NORMRET"],
            learning_rate=cfg["LR"],
            lstm_units=cfg["LSTM_UNITS"]
        )

    model.fit(
        X_tr_s,
        {"return": y_ret_tr, "direction": y_dir_tr},
        epochs=80,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ å·²å„²å­˜æ¨¡å‹ï¼š{MODEL_PATH}")

    pred_ret, pred_dir = model.predict(X_te_s, verbose=0)
    raw_norm_returns = pred_ret[-1]  # âœ… normalized returnsï¼ˆå·²é™å¹…ï¼‰

    print(f"ğŸ“ˆ é æ¸¬æ–¹å‘æ©Ÿç‡ï¼ˆçœ‹æ¼²ï¼‰: {pred_dir[-1][0]:.2%}")

    asof_date = df.index.max()
    last_close = float(df.loc[asof_date, "Close"])

    # âœ… æŠŠ normalized return ä¹˜å›æ³¢å‹•å°ºåº¦ï¼ˆç”¨ asof çš„ RET_STD_20ï¼‰
    scale_last = float(df.loc[asof_date, "RET_STD_20"])
    if not np.isfinite(scale_last) or scale_last <= 0:
        # fallbackï¼šç”¨æœ€è¿‘ 20 å¤© std ä¼°
        scale_last = float(np.log(df["Close"].astype(float)).diff().rolling(20).std().iloc[-1])
    scale_last = max(scale_last, 1e-6)


    # ğŸ”§ ADD: Regime-based æ³¢æ®µæ”¾å¤§ / å£“ç¸®ï¼ˆç”¨æœ€è¿‘çš„ TREND_60ï¼‰
    trend60 = last_valid_value(df, "TREND_60", lookback=5)
    
    amp = 1.0
    if trend60 is not None:
        if trend60 > 1.0:
            amp = 1.4      # å¼·è¶¨å‹¢ â†’ æ”¾å¤§
        elif trend60 < -1.0:
            amp = 1.3      # å¼·ç©ºè¶¨å‹¢
        elif abs(trend60) < 0.5:
            amp = 0.6      # ç›¤æ•´ â†’ å£“ç¸®
    
    print(f"ğŸ“Š Regime amp = {amp:.2f}")

    prices = []
    price = last_close
    for r_norm in raw_norm_returns:
        r = float(r_norm) * scale_last * amp
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    future_df["date"] = pd.bdate_range(
        start=df.index.max() + BDay(1),
        periods=STEPS
    )

    # âœ… é æ¸¬æ•¸å€¼è¼¸å‡º CSVï¼ˆæª”åå« tickerï¼‰
    os.makedirs("results", exist_ok=True)
    forecast_csv = f"results/{datetime.now():%Y-%m-%d}_{TICKER}_forecast.csv"
    future_df.to_csv(forecast_csv, index=False, encoding="utf-8-sig")

    # âœ… åœ–è¼¸å‡ºï¼ˆå…§å®¹ä¸å‹•ã€æª”åæ”¹å« tickerï¼‰
    plot_and_save(df, future_df, ticker=TICKER)
    plot_backtest_error(df, ticker=TICKER)
    # ================= 6M Trend Forecastï¼ˆx è»¸ = æœˆï¼‰ =================
    plot_6m_trend_advanced(
        df=df,
        last_close=last_close,
        raw_norm_returns=raw_norm_returns,
        scale_last=scale_last,
        ticker=TICKER,
        asof_date=asof_date
    )

#8110stock 


# -*- coding: utf-8 -*-
"""
FireBase_Attention_LSTM_Direction.py  (8110stock.py)
- Attention-LSTM
- Multi-task: Return path + Direction
- âœ… å°è³‡æ–™å‹å–„ç‰ˆï¼šæ›´ç©©ã€æ›´ä¸å®¹æ˜“äº‚å™´
  1) LOOKBACK=40, STEPS=5
  2) LSTM + Attention poolingï¼ˆåƒæ•¸æ¯” Transformer æ›´é©åˆå°è³‡æ–™ï¼‰
  3) âœ… Return head åŠ  tanh é™å¹…ï¼ˆé¿å…é æ¸¬çˆ†ç‚¸ï¼‰
  4) âœ… Volume åš log1pï¼ˆå°è³‡æ–™æ›´ç©©ï¼‰ 
- åœ–è¡¨è¼¸å‡ºå®Œå…¨ä¸è®Šï¼ˆä¿ç•™ Today æ¨™è¨˜ï¼‰

âœ… æ”¹1ï¼šä¿®æ­£ scaler fit / split åº§æ¨™ç³»ï¼Œé¿å…è³‡æ–™æ´©æ¼ï¼ˆleakageï¼‰
  - create_sequences å›å‚³æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æ—¥æœŸ idx
  - split ç”¨æ¨£æœ¬æ•¸åˆ‡ï¼Œscaler.fit åªç”¨ train å€é–“çš„ df ç‰¹å¾µ

âœ… æ–°å¢ï¼šåŒæ™‚è¼¸å‡º PNG + CSVï¼ˆæª”åå« tickerï¼‰
  - results/YYYY-MM-DD_TICKER_pred.png
  - results/YYYY-MM-DD_TICKER_forecast.csv
  - results/YYYY-MM-DD_TICKER_backtest.png
  - results/YYYY-MM-DD_TICKER_backtest.csv

âœ… è¯æ± 8110.TW å°ˆå±¬å¼·åŒ–ï¼ˆç…§å‰é¢å»ºè­°æ”¹ï¼‰
  A) Featureï¼šåŠ å…¥ HL_RANGE / GAP / VOL_RELï¼ˆæ›´è²¼è¿‘ä¸­å°å‹è‚¡/æ³¢å‹•è‚¡ï¼‰
  B) Targetï¼šé æ¸¬ã€Œæ³¢å‹•æ¨™æº–åŒ–ã€log-returnï¼ˆç”¨ t-1 çš„ RET_STD_20 åšå°ºåº¦ï¼Œé¿å…å·çœ‹ï¼‰
  C) æ¨å›åƒ¹æ ¼æ™‚ï¼šæŠŠé æ¸¬çš„ normalized return ä¹˜å› asof çš„ RET_STD_20
  D) loss_weightsï¼šdirection æ¬Šé‡æé«˜ï¼ˆæ–¹å‘é€šå¸¸æ¯”ç²¾æº–åƒ¹æ›´å¯é ï¼‰
"""

import os, json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout,
    Softmax, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping

from zoneinfo import ZoneInfo
now_tw = datetime.now(ZoneInfo("Asia/Taipei"))

# Firebase
import firebase_admin
from firebase_admin import credentials, firestore

# ================= Firebase åˆå§‹åŒ– =================
key_dict = json.loads(os.environ.get("FIREBASE", "{}"))
db = None

if key_dict:
    cred = credentials.Certificate(key_dict)
    try:
        firebase_admin.get_app()
    except Exception:
        firebase_admin.initialize_app(cred)
    db = firestore.client()

# ================= Firestore è®€å– =================
def load_df_from_firestore(
    ticker,
    collection="NEW_stock_data_liteon",
    days=500
):
    if db is None:
        raise ValueError("âŒ Firestore æœªåˆå§‹åŒ–")

    rows = []

    for doc in db.collection(collection).stream():
        p = doc.to_dict().get(ticker)
        if p:
            rows.append({
                "date": doc.id,   # YYYY-MM-DD
                **p
            })

    if not rows:
        raise ValueError("âš ï¸ Firestore ç„¡è³‡æ–™")

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["date"])

    # âœ… é€™è£¡æ‰æ˜¯ã€Œé˜²å‡æ—¥çš„ç¬¬ä¸€é“é–€ã€
    df = (
        df.sort_values("date")
          .tail(days)          # åªä¿ç•™æœ€è¿‘ N ç­†ã€Œäº¤æ˜“æ—¥ã€
          .set_index("date")
    )
    return df



# ================= å‡æ—¥è£œä»Šå¤© =================
def ensure_latest_trading_row(df):
    """
    è‹¥ä»Šå¤©æ˜¯éäº¤æ˜“æ—¥ï¼Œè£œ rowï¼ˆforward fillï¼‰
    ä½† Close ä¸æœƒè®Šï¼Œç”¨æ–¼ã€Œé æ¸¬ today+1ã€
    """
    today = pd.Timestamp(datetime.now().date())
    last = df.index.max()

    if last.normalize() >= today:
        return df

    all_days = pd.bdate_range(last, today)

    for d in all_days[1:]:
        if d not in df.index:
            df.loc[d] = df.loc[last]

    return df.sort_index()


def get_asof_trading_day(df: pd.DataFrame):
    """
    å›å‚³ (asof_date, is_today_trading)
    - è‹¥ä»Šå¤©æ˜¯äº¤æ˜“æ—¥ â†’ ç”¨ä»Šå¤©
    - è‹¥ä»Šå¤©éäº¤æ˜“æ—¥ â†’ ç”¨æœ€è¿‘ä¸€å€‹äº¤æ˜“æ—¥
    """
    today = pd.Timestamp(datetime.now().date())
    last_trading_day = df.index.max()

    if last_trading_day.normalize() == today:
        return last_trading_day, True
    else:
        return last_trading_day, False



# ================= Feature Engineeringï¼ˆè¯æ±å°ˆå±¬ï¼‰ =================
def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # âœ… Volume å°ºåº¦ç©©å®šï¼ˆéå¸¸å»ºè­°ï¼‰
    if "Volume" in df.columns:
        df["Volume"] = np.log1p(df["Volume"].astype(float))

    # åœ–è¡¨ç”¨å‡ç·šï¼ˆä¿æŒä¸è®Šï¼‰
    df["SMA5"] = df["Close"].rolling(5).mean()
    df["SMA10"] = df["Close"].rolling(10).mean()

    # âœ… è¯æ±ï¼ˆæ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰ç‰¹å¾µ
    # éœ€è¦ Firestore æœ‰ Open/High/Lowï¼ˆä½  catch_stock.py å¯«å…¥æ˜¯æœ‰çš„ï¼‰
    if all(c in df.columns for c in ["Open", "High", "Low", "Close"]):
        df["HL_RANGE"] = (df["High"].astype(float) - df["Low"].astype(float)) / df["Close"].astype(float)
        df["GAP"] = (df["Open"].astype(float) - df["Close"].shift(1).astype(float)) / df["Close"].shift(1).astype(float)
    else:
        # è‹¥ç¼ºæ¬„ä½ï¼Œå…ˆçµ¦ NaNï¼ˆå¾Œé¢ dropna æœƒæ’æ‰ï¼‰
        df["HL_RANGE"] = np.nan
        df["GAP"] = np.nan

    # âœ… é‡èƒ½ç›¸å°å¼·å¼±ï¼ˆç”¨ log1p ä¹‹å¾Œçš„ Volume å»åšæ¯”å€¼å³å¯ï¼‰
    df["VOL_REL"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)

    # âœ… 20æ—¥æ³¢å‹•ï¼ˆç”¨ä¾†æ¨™æº–åŒ– yï¼‰
    close = df["Close"].astype(float)
    df["RET_STD_20"] = np.log(close).diff().rolling(20).std()

    # ğŸ”§ ADD: Regime / æ³¢æ®µç‹€æ…‹ç‰¹å¾µï¼ˆä¸å­˜ Firebaseï¼‰
    ma60 = df["Close"].rolling(60)
    df["TREND_60"] = (df["Close"] - ma60.mean()) / (ma60.std() + 1e-9)
    
    df["TREND_SLOPE_20"] = (
        df["Close"].rolling(20).mean().diff()
    ) / df["Close"]
    
    return df


# ================= Sequenceï¼ˆæ¨™æº–åŒ– returnï¼Œé¿å…æ³¢å‹• regime å½±éŸ¿ï¼‰ =================
def create_sequences(
    df, features,
    steps=5, window=40,
    trend_h=20,           # âœ… æ–°å¢ï¼šè¶¨å‹¢ horizonï¼ˆäº¤æ˜“æ—¥ï¼‰
    k_flat=0.8,           # âœ… æ–°å¢ï¼šç›¤æ•´é–€æª»ï¼ˆè¶Šå¤§è¶Šä¿å®ˆï¼‰
    eps=1e-9
):
    """
    X: t-window ~ t-1
    y_ret: t ~ t+steps-1 normalized log return (ç”¨ t-1 æ³¢å‹•åšå°ºåº¦)
    y_dir: æœªä¾† steps å¤©ç´¯ç©æ–¹å‘ï¼ˆäºŒåˆ†é¡ï¼Œä¿ç•™çµ¦çŸ­ç·šï¼‰
    y_trend3: æœªä¾† trend_h å¤©è¶¨å‹¢ï¼ˆä¸‰åˆ†é¡ Up/Flat/Downï¼‰âœ… æ›´è²¼è¿‘çœŸå¯¦
      - ç”¨æ³¢å‹•é–€æª»ï¼š|cumret| < k_flat * scale * sqrt(trend_h) => Flat
    idx: æ¯å€‹æ¨£æœ¬å°æ‡‰ t ç•¶å¤©æ—¥æœŸ
    """
    X, y_ret, y_dir, y_trend3, idx = [], [], [], [], []

    close = df["Close"].astype(float)
    logret = np.log(close).diff()

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    feat = df[features].values

    # éœ€è¦åŒæ™‚æ»¿è¶³ steps èˆ‡ trend_h çš„æœªä¾†è³‡æ–™
    max_h = max(steps, trend_h)

    for i in range(window, len(df) - max_h):
        x_seq = feat[i - window:i]
        if np.any(np.isnan(x_seq)):
            continue

        # âœ… ç”¨ t-1 æ³¢å‹•å°ºåº¦ï¼ˆé¿å…å·çœ‹ï¼‰
        scale = df["RET_STD_20"].iloc[i - 1]
        if pd.isna(scale) or scale < eps:
            continue
        scale = float(scale) + eps

        # ---------- 5D return head ----------
        future_ret_raw_5d = logret.iloc[i:i + steps].values
        if np.any(np.isnan(future_ret_raw_5d)):
            continue
        future_ret_norm_5d = future_ret_raw_5d / scale

        # çŸ­ç·šæ–¹å‘ï¼ˆäºŒåˆ†é¡ï¼Œä¿ç•™ï¼‰
        dir_5d = 1.0 if future_ret_raw_5d.sum() > 0 else 0.0

        # ---------- 20D trend head (3-class) ----------
        future_ret_raw_tr = logret.iloc[i:i + trend_h].values
        if np.any(np.isnan(future_ret_raw_tr)):
            continue

        cum = float(future_ret_raw_tr.sum())  # logç´¯ç©
        # âœ… ç›¤æ•´é–€æª»ï¼šæ³¢å‹• * sqrt(h)
        thr = float(k_flat) * scale * np.sqrt(float(trend_h))

        # class: 0=Down, 1=Flat, 2=Up
        if cum > thr:
            cls = 2
        elif cum < -thr:
            cls = 0
        else:
            cls = 1

        onehot = np.zeros(3, dtype=np.float32)
        onehot[cls] = 1.0

        X.append(x_seq)
        y_ret.append(future_ret_norm_5d)
        y_dir.append(dir_5d)
        y_trend3.append(onehot)
        idx.append(df.index[i])

    return (
        np.array(X),
        np.array(y_ret),
        np.array(y_dir),
        np.array(y_trend3),
        np.array(idx)
    )

def build_attention_lstm(
    input_shape,
    steps,
    max_daily_normret=3.0,
    learning_rate=6e-4,
    lstm_units=64
):
    inp = Input(shape=input_shape)

    x = LSTM(lstm_units, return_sequences=True)(inp)
    x = Dropout(0.2)(x)

    score = Dense(1, name="attn_score")(x)
    weights = Softmax(axis=1, name="attn_weights")(score)
    context = Lambda(lambda t: tf.reduce_sum(t[0] * t[1], axis=1),
                     name="attn_context")([x, weights])

    # âœ… return headï¼štanh é™å¹…ï¼ˆnormalized returnï¼‰
    raw = Dense(steps, activation="tanh")(context)           # [-1, 1]
    out_ret = Lambda(lambda t: t * max_daily_normret, name="return")(raw)

    # âœ… 5D directionï¼ˆçŸ­ç·šï¼‰
    out_dir = Dense(1, activation="sigmoid", name="direction")(context)

    # âœ… 20D trendï¼ˆä¸‰åˆ†é¡ï¼šDown/Flat/Upï¼‰â†’ æ›´è²¼è¿‘çœŸå¯¦
    out_trend = Dense(3, activation="softmax", name="trend3")(context)

    model = Model(inp, [out_ret, out_dir, out_trend])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss={
            "return": tf.keras.losses.Huber(),
            "direction": "binary_crossentropy",
            "trend3": "categorical_crossentropy"
        },
        # âœ… è¶¨å‹¢æ¯”çŸ­ç·šæ–¹å‘æ›´é‡è¦ï¼ˆæ›´è²¼è¿‘çœŸå¯¦ï¼‰
        loss_weights={
            "return": 1.0,
            "direction": 0.4,
            "trend3": 1.2
        },
        metrics={
            "direction": [
                tf.keras.metrics.BinaryAccuracy(name="acc"),
                tf.keras.metrics.AUC(name="auc")
            ],
            "trend3": [
                tf.keras.metrics.CategoricalAccuracy(name="acc")
            ]
        }
    )
    return model

# ================= åŸé æ¸¬åœ–ï¼ˆå…§å®¹ä¸å‹•ï¼šæ–°å¢ Today æ¨™è¨˜ï¼‰ =================
def plot_and_save(df_hist, future_df, ticker: str):
    hist = df_hist.tail(10)
    hist_dates = hist.index.strftime("%m-%d").tolist()
    future_dates = future_df["date"].dt.strftime("%m-%d").tolist()

    all_dates = hist_dates + future_dates
    x_hist = np.arange(len(hist_dates))
    x_future = np.arange(len(hist_dates), len(all_dates))

    plt.figure(figsize=(18,8))
    ax = plt.gca()

    ax.plot(x_hist, hist["Close"], label="Close")
    ax.plot(x_hist, hist["SMA5"], label="SMA5")
    ax.plot(x_hist, hist["SMA10"], label="SMA10")

    # âœ… Today é»èˆ‡æ–‡å­—ï¼ˆhist æœ€å¾Œä¸€å€‹é»ï¼‰
    today_x = x_hist[-1]
    today_y = float(hist["Close"].iloc[-1])
    ax.scatter([today_x], [today_y], marker="*", s=160, label="Today Close")
    ax.text(today_x, today_y + 0.3, f"Today {today_y:.2f}",
            fontsize=17, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["Close"].iloc[-1]] + future_df["Pred_Close"].tolist(),
        "r:o", label="Pred Close"
    )

    for i, price in enumerate(future_df["Pred_Close"]):
        ax.text(x_future[i], price + 0.3, f"{price:.2f}",
                color="red", fontsize=15, ha="center")

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA5"].iloc[-1]] + future_df["Pred_MA5"].tolist(),
        "g--o", label="Pred MA5"
    )

    ax.plot(
        np.concatenate([[x_hist[-1]], x_future]),
        [hist["SMA10"].iloc[-1]] + future_df["Pred_MA10"].tolist(),
        "b--o", label="Pred MA10"
    )

    ax.set_xticks(np.arange(len(all_dates)))
    ax.set_xticklabels(all_dates, rotation=45, ha="right", fontsize=15)
    ax.legend()
    ax.set_title("2301.TW Attention-LSTM é æ¸¬")  # âœ… å…§å®¹ä¸å‹•

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{datetime.now():%Y-%m-%d}_{ticker}_pred.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

# ================= å›æ¸¬æ±ºç­–åˆ†å²”åœ–ï¼ˆPNG + CSVï¼‰ =================
def plot_backtest_error(df, ticker: str):
    """
    æ±ºç­–å¼å›æ¸¬åœ–ï¼ˆDecision-based Backtestï¼‰
    - è‡ªå‹•æ’é™¤ä»Šå¤©çš„ forecast
    - ä½¿ç”¨æœ€è¿‘ä¸€ç­†æ­·å² forecastï¼ˆåŒ tickerï¼‰
    """
        # === åªä¿ç•™çœŸå¯¦äº¤æ˜“æ—¥ï¼ˆæ’é™¤ ensure_latest_trading_row è£œçš„å‡æ—¥ï¼‰===
    real_df = df.copy()
    real_df = real_df[real_df["Close"].diff().abs() > 1e-9]

    today = pd.Timestamp(datetime.now().date())

    if not os.path.exists("results"):
        print("âš ï¸ ç„¡ results è³‡æ–™å¤¾ï¼Œç•¥éå›æ¸¬")
        return

    forecast_files = []
    for f in os.listdir("results"):
        if not f.endswith(f"_{ticker}_forecast.csv"):
            continue
        try:
            d = pd.to_datetime(f.split("_")[0])
        except Exception:
            continue
        if d < today:
            forecast_files.append((d, f))

    if not forecast_files:
        print("âš ï¸ æ‰¾ä¸åˆ°å¯ç”¨çš„æ­·å² forecastï¼ˆå·²æ’é™¤ä»Šå¤© & å·²é™å®š tickerï¼‰")
        return

    forecast_files.sort(key=lambda x: x[0], reverse=True)
    forecast_date, forecast_name = forecast_files[0]
    forecast_csv = os.path.join("results", forecast_name)

    print(f"ğŸ“„ Backtest ä½¿ç”¨ forecastï¼š{forecast_name}")

    future_df = pd.read_csv(forecast_csv, parse_dates=["date"])

        # === ç”¨ã€ŒçœŸå¯¦äº¤æ˜“æ—¥ã€æ±ºå®š t / t+1 ===
    valid_days = real_df.index[real_df.index < today]

    if len(valid_days) < 2:
        print("âš ï¸ ç„¡è¶³å¤ çœŸå¯¦äº¤æ˜“æ—¥ï¼Œç•¥éå›æ¸¬")
        return

    # t = æœ€å¾Œä¸€å€‹å¯æ±ºç­–æ—¥
    # t1 = çœŸæ­£ç™¼ç”Ÿçš„ä¸‹ä¸€å€‹äº¤æ˜“æ—¥
    t = valid_days[-2]
    t1 = valid_days[-1]


    close_t = float(real_df.loc[t, "Close"])
    pred_t1 = float(future_df.loc[0, "Pred_Close"])
    actual_t1 = float(real_df.loc[t1, "Close"])
    

    trend = df.loc[:t].tail(4)
    x_trend = np.arange(len(trend))
    x_t = x_trend[-1]

    plt.figure(figsize=(14, 6))
    ax = plt.gca()

    ax.plot(x_trend, trend["Close"], "k-o", label="Recent Close")
    ax.plot([x_t, x_t + 1], [close_t, pred_t1], "r--o", linewidth=2.5, label="Pred (t â†’ t+1)")
    ax.plot([x_t, x_t + 1], [close_t, actual_t1], "g-o", linewidth=2.5, label="Actual (t â†’ t+1)")

    dx = 0.08
    price_offset = max(0.2, close_t * 0.002)

    ax.text(x_t, close_t + price_offset, f"{close_t:.2f}", ha="center", va="bottom", fontsize=18, color="black")
    ax.text(x_t + 1 + dx, pred_t1, f"Pred {pred_t1:.2f}", ha="left", va="center", fontsize=16, color="red")
    ax.text(x_t + 1 + dx, actual_t1, f"Actual {actual_t1:.2f}", ha="left", va="center", fontsize=16, color="green")

    labels = trend.index.strftime("%m-%d").tolist()
    labels.append(t1.strftime("%m-%d"))
    ax.set_xticks(np.arange(len(labels)))
    ax.set_xticklabels(labels)

    ax.set_title("2301.TW Decision Backtest (t â†’ t+1)")  # âœ… å…§å®¹ä¸å‹•
    ax.legend()
    ax.grid(alpha=0.3)

    ax.text(
        0.01, 0.01,
        f"Generated at {now_tw:%Y-%m-%d %H:%M:%S} (TW)",
        transform=ax.transAxes,
        fontsize=8,
        alpha=0.4,
        ha="left",
        va="bottom"
    )

    os.makedirs("results", exist_ok=True)
    out_png = f"results/{today:%Y-%m-%d}_{ticker}_backtest.png"
    plt.savefig(out_png, dpi=300, bbox_inches="tight")
    plt.close()

    bt = pd.DataFrame([{
        "forecast_date": forecast_date.date(),
        "decision_day": t.date(),
        "close_t": close_t,
        "pred_t1": pred_t1,
        "actual_t1": actual_t1,
        "direction_pred": int(np.sign(pred_t1 - close_t)),
        "direction_actual": int(np.sign(actual_t1 - close_t))
    }])

    out_csv = f"results/{today:%Y-%m-%d}_{ticker}_backtest.csv"
    bt.to_csv(out_csv, index=False, encoding="utf-8-sig")

import glob

def plot_6m_trend_advanced(
    df: pd.DataFrame,
    last_close: float,
    raw_norm_returns: np.ndarray,
    scale_last: float,
    ticker: str,
    asof_date: pd.Timestamp,
    amp: float = 1.0,
    pred_ret_all=None,          # å¯é¸ï¼šå‚³å…¥ pred_ret å…¨éƒ¨ (N, STEPS)
    pred_dir_last=None,         # å¯é¸ï¼šå‚³å…¥æœ€å¾Œä¸€ç­†æ–¹å‘æ©Ÿç‡ (float, 0~1)
    k_ens: int = 20
):
    """
    8110-tuned 6M Outlook (æ›´è²¼è¿‘ç¾å¯¦ç‰ˆ)
    - ä¸å†æŠŠ 5æ—¥é æ¸¬ç¡¬è¤‡åˆ© 21æ¬¡ â†’ æ”¹æˆã€Œæ¨¡å‹åªèª¿ drift çš„ edgeã€
    - ç”¨ pred_dir ä¿¡å¿ƒèª¿æ•´æ¨¡å‹å½±éŸ¿åŠ›ï¼ˆä¸ç¢ºå®šå°±å›æ­¸ä¿å®ˆï¼‰
    - Expected Range ç”¨æ­·å² backtest èª¤å·®æ ¡æº–ï¼ˆæœ‰æª”å°±ç”¨ï¼Œæ²’æœ‰å°± fallback ATRï¼‰
    """
    MONTHS = 6
    DPM = 21
    eps = 1e-9

    # -----------------------------
    # 0) å–æ¨¡å‹ 5æ—¥è¼¸å‡ºï¼ˆensemble æ›´ç©©ï¼‰
    # -----------------------------
    if pred_ret_all is not None:
        try:
            K = min(int(k_ens), len(pred_ret_all))
            base5 = np.median(np.asarray(pred_ret_all)[-K:], axis=0).astype(float)
        except Exception:
            base5 = np.array(raw_norm_returns, dtype=float)
    else:
        base5 = np.array(raw_norm_returns, dtype=float)

    if base5 is None or len(base5) == 0:
        raise ValueError("âŒ base5 ç‚ºç©ºï¼šraw_norm_returns/pred_ret_all ç„¡æ³•ä½¿ç”¨")

    # -----------------------------
    # 1) å…ˆç®—æ­·å² driftï¼ˆç”¨ log-return æ›´ç©©ï¼‰
    # -----------------------------
    close = df["Close"].astype(float)
    logp = np.log(close + eps)
    ret = logp.diff()

    daily_drift = float(ret.ewm(span=60).mean().tail(20).mean())
    daily_drift = float(np.clip(daily_drift, -0.01, 0.01))  # é˜²çˆ†

    # -----------------------------
    # 2) Regimeï¼šRSI/ATR â†’ trend_scoreï¼ˆæ§åˆ¶ driftï¼‰
    # -----------------------------
    atr = last_valid_value(df, "ATR_14", lookback=40)
    rsi = last_valid_value(df, "RSI", lookback=40)

    if atr is None:
        raise ValueError("âŒ ç„¡å¯ç”¨ ATR_14ï¼ˆæœ€è¿‘ 40 æ—¥çš†ç‚º NaNï¼‰")

    atr_ratio = float(atr) / float(last_close + eps)
    vol_regime = atr_ratio

    trend_score = 1.0
    if rsi is not None and rsi > 75:
        trend_score *= 0.35
    elif rsi is not None and rsi > 65:
        trend_score *= 0.65

    if vol_regime < 0.015:
        trend_score *= 0.6
    if vol_regime > 0.08:
        trend_score *= 0.75

    # -----------------------------
    # 3) âœ… æ¨¡å‹ edgeï¼šåªç”¨ä¾†ã€Œèª¿ driftã€ï¼Œä¸å† 21 æ¬¡è¤‡åˆ©æ¨ 1M
    # -----------------------------
    # base5 æ˜¯ normalized return â†’ ä¹˜å› scale_last è®Šæˆæ—¥ log-return edge
    edge_daily = float(np.mean(base5)) * float(scale_last)

    # amp ä¸è¦ç›´æ¥æ”¾å¤§ edgeï¼ˆé¿å…å™´ï¼‰ï¼›åªå°é€±æœŸéœ‡ç›ªå¯æ”¾å¤§
    # RSI éç†±æ™‚ï¼Œedge å†å£“ä¸€é»ï¼ˆæ›´è²¼è¿‘ç¾å¯¦ï¼‰
    if rsi is not None and rsi > 75:
        edge_daily *= 0.6

    # âœ… capï¼š8110 å–®æ—¥ edge Â±0.4% å·²ç¶“å¾ˆå¯¬
    edge_daily = float(np.clip(edge_daily, -0.004, 0.004))

    # æœ€çµ‚ driftï¼ˆåŠ ä¸Šæ¨¡å‹ edgeï¼Œå†ä¹˜ trend_scoreï¼‰
    daily_drift_adj = (daily_drift + edge_daily) * trend_score
    daily_drift_adj = float(np.clip(daily_drift_adj, -0.01, 0.01))
    monthly_logret = daily_drift_adj * DPM

    # 1M anchorï¼ˆæ›´ç©©ã€æ›´åƒç¾å¯¦ï¼‰
    model_1m_price = float(last_close * np.exp(monthly_logret))

    # -----------------------------
    # 4) FFT é€±æœŸï¼šç”¨ log-return åšï¼ˆé¿å…è¶¨å‹¢è¢«ç•¶é€±æœŸï¼‰
    # -----------------------------
    r = ret.dropna().iloc[-180:].values
    if len(r) < 60:
        cycle_p = 80
    else:
        r_centered = r - r.mean()
        fft_p = np.fft.rfft(r_centered)
        freq_p = np.fft.rfftfreq(len(r_centered), d=1)
        mag = np.abs(fft_p)
        mag[0] = 0.0
        idx_p = int(np.argmax(mag))
        if idx_p == 0 or freq_p[idx_p] <= 1e-6:
            cycle_p = 80
        else:
            cycle_p = int(round(1 / freq_p[idx_p]))
            cycle_p = int(np.clip(cycle_p, 40, 120))

    vol_series = df["Volume"].iloc[-180:].dropna().astype(float).values
    if len(vol_series) < 60:
        cycle_v = 30
    else:
        v_centered = vol_series - vol_series.mean()
        fft_v = np.fft.rfft(v_centered)
        freq_v = np.fft.rfftfreq(len(v_centered), d=1)
        magv = np.abs(fft_v)
        magv[0] = 0.0
        idx_v = int(np.argmax(magv))
        if idx_v == 0 or freq_v[idx_v] <= 1e-6:
            cycle_v = 35
        else:
            cycle_v = int(round(1 / freq_v[idx_v]))
            cycle_v = int(np.clip(cycle_v, 20, 60))

    # -----------------------------
    # 5) éœ‡ç›ªå¹…åº¦ base_ampï¼ˆç”± ATR%ï¼ŒRSIéç†±ä¸è¦æ”¾å¤§ï¼‰
    # -----------------------------
    if rsi is None:
        rsi = 50.0
    rsi_strength = abs(float(rsi) - 50.0) / 50.0
    rsi_factor = np.clip(0.6 + 0.8 * rsi_strength, 0.7, 1.25)
    if rsi > 75:
        rsi_factor *= 0.75  # éç†±å£“ç¸®éœ‡ç›ª

    base_amp = float(np.clip(atr_ratio * rsi_factor, 0.02, 0.18))
    # âœ… amp åªç”¨ä¾†èª¿é€±æœŸéœ‡ç›ªï¼ˆä¸æ˜¯èª¿æ¨¡å‹ driftï¼‰
    base_amp = float(np.clip(base_amp * float(amp), 0.02, 0.22))

    # -----------------------------
    # 6) drift åŸºæº–ç·šï¼ˆä¸ç”¨è¦†è“‹ trend[0]ï¼Œé¿å…é›™é‡ anchorï¼‰
    # -----------------------------
    trend = []
    p = float(last_close)
    for _ in range(MONTHS):
        p *= np.exp(monthly_logret)
        trend.append(p)
    trend = np.array(trend, dtype=float)

    # -----------------------------
    # 7) âœ… ç”¨ pred_dir ä¿¡å¿ƒèª¿æ•´æ¨¡å‹å½±éŸ¿åŠ› wï¼ˆæ›´åƒç¾å¯¦ï¼‰
    # -----------------------------
    if pred_dir_last is None:
        conf = 0.35  # ä¸çŸ¥é“ä¿¡å¿ƒ â†’ ä¿å®ˆ
    else:
        try:
            pdv = float(pred_dir_last)
            conf = abs(pdv - 0.5) * 2.0  # 0~1
            conf = float(np.clip(conf, 0.0, 1.0))
        except Exception:
            conf = 0.35

    prices = [float(last_close)]
    for m in range(1, MONTHS + 1):
        phase_p = 2 * np.pi * (m * DPM) / float(cycle_p)
        phase_v = 2 * np.pi * (m * DPM) / float(cycle_v)

        cycle_main = base_amp * np.sin(phase_p)
        cycle_pull = 0.6 * base_amp * np.sin(phase_v + np.pi)

        # æœˆä»½è¶Šé è¶Šä¸ä¿¡æ¨¡å‹ï¼›conf è¶Šä½ä¹Ÿè¶Šä¸ä¿¡
        w_time = float(np.exp(-0.55 * (m - 1)))
        w_conf = 0.25 + 0.75 * conf
        w = float(np.clip(w_time * w_conf, 0.05, 0.90))

        center = w * model_1m_price + (1 - w) * float(trend[m - 1])
        price = center * (1 + cycle_main + cycle_pull)
        prices.append(float(price))

    prices = np.array(prices, dtype=float)

    # -----------------------------
    # 8) âœ… Expected Rangeï¼šç”¨ä½ è‡ªå·± backtest èª¤å·®æ ¡æº–ï¼ˆæ›´è²¼è¿‘çœŸå¯¦ï¼‰
    # -----------------------------
    def load_recent_price_errors(ticker, max_files=90):
        files = sorted(glob.glob(f"results/*_{ticker}_backtest.csv"))[-max_files:]
        errs = []
        for f in files:
            try:
                bt = pd.read_csv(f)
                # èª¤å·®ï¼šactual - predï¼ˆåƒ¹æ ¼å·®ï¼‰
                e = float(bt["actual_t1"].iloc[0]) - float(bt["pred_t1"].iloc[0])
                if np.isfinite(e):
                    errs.append(e)
            except Exception:
                pass
        return np.array(errs, dtype=float)

    errs = load_recent_price_errors(ticker)
    t = np.arange(len(prices), dtype=float)
    scale_t = np.sqrt(np.maximum(t, 1.0))  # âˆšt æ“´æ•£

    if len(errs) >= 20:
        q10, q90 = np.quantile(errs, [0.10, 0.90])
        # ç”¨å›æ¸¬èª¤å·®ä¾†æ“´æ•£ bandï¼ˆåƒ¹æ ¼å·®ï¼‰
        upper = prices + float(q90) * scale_t
        lower = prices + float(q10) * scale_t
    else:
        # fallbackï¼šç”¨ ATR åš bandï¼ˆè¼ƒç²—ï¼Œä½†ä¸æœƒäº‚ï¼‰
        upper = prices * (1 + base_amp * (0.6 + 0.7 * (scale_t / scale_t.max())))
        lower = prices * (1 - base_amp * (0.6 + 0.7 * (scale_t / scale_t.max())))

    # -----------------------------
    # 9) X label
    # -----------------------------
    labels = ["Now"] + [
        (asof_date + pd.DateOffset(months=i)).strftime("%Y-%m")
        for i in range(1, MONTHS + 1)
    ]

    # -----------------------------
    # 10) Plot
    # -----------------------------
    plt.figure(figsize=(15, 7))
    x = np.arange(MONTHS + 1)

    plt.fill_between(x, lower, upper, alpha=0.18, label="Expected Range")
    plt.plot(x, prices, "r-o", linewidth=2.8, label="Projected Path")
    plt.scatter(0, prices[0], s=180, marker="*", label="Today")

    for i, p in enumerate(prices[1:]):
        plt.text(i + 1, p, f"{p:.2f}", ha="center", fontsize=12)

    info = (
        f"asof={asof_date.date()} | model_1M={model_1m_price:.2f} | amp={amp:.2f} | conf={conf:.2f}\n"
        f"drift(d)={daily_drift_adj:.5f} | trend_score={trend_score:.2f} | ATR%={atr_ratio:.2%} | RSI={float(rsi):.2f}\n"
        f"cycle_p={cycle_p} | cycle_v={cycle_v} | base_amp={base_amp:.3f} | edge(d)={edge_daily:.4f}"
    )
    plt.gca().text(
        0.01, 0.02, info,
        transform=plt.gca().transAxes,
        fontsize=9,
        alpha=0.55,
        ha="left",
        va="bottom"
    )

    plt.xticks(x, labels, fontsize=13)
    plt.title(f"{ticker} Â· 6M Outlook (Realistic 8110: drift+edge, calibrated band)")
    plt.grid(alpha=0.3)
    plt.legend()

    os.makedirs("results", exist_ok=True)
    out = f"results/{datetime.now():%Y-%m-%d}_{ticker}_6m_advanced.png"
    plt.savefig(out, dpi=300, bbox_inches="tight")
    plt.close()


def last_valid_value(df: pd.DataFrame, col: str, lookback: int = 30):
    """
    å–æœ€è¿‘ä¸€ç­†æœ‰æ•ˆï¼ˆé NaNï¼‰çš„æŒ‡æ¨™å€¼
    - ç”¨æ–¼éäº¤æ˜“æ—¥ / è£œ today row çš„æƒ…æ³
    """
    if col not in df.columns:
        return None

    s = df[col].iloc[-lookback:]
    s = s[s.notna()]
    if s.empty:
        return None
    return float(s.iloc[-1])



# ================= Main =================
# ================= Main =================
if __name__ == "__main__":
    TICKER = "8110.TW"
    COLLECTION = "NEW_stock_data_liteon"

    # âœ… è¯æ±å°ˆå±¬è¨­å®šï¼ˆnormalized return ç‰ˆæœ¬ï¼‰
    STOCK_CONFIG = {
        "8110.TW": {
            "LOOKBACK": 40,
            "STEPS": 5,                 # 5æ—¥ï¼šreturn head ç”¨
            "MAX_DAILY_NORMRET": 3.0,   # normalized return é™å¹…ï¼ˆ2~4 å¸¸è¦‹ï¼‰
            "LR": 6e-4,
            "LSTM_UNITS": 64
        },
    }

    cfg = STOCK_CONFIG.get(TICKER, {
        "LOOKBACK": 40,
        "STEPS": 5,
        "MAX_DAILY_NORMRET": 3.0,
        "LR": 6e-4,
        "LSTM_UNITS": 64
    })

    LOOKBACK = cfg["LOOKBACK"]
    STEPS = cfg["STEPS"]

    # âœ… è¶¨å‹¢ head è¨­å®šï¼ˆæœ€æœ‰æ„Ÿï¼‰
    TREND_H = 20      # 20 äº¤æ˜“æ—¥ â‰ˆ 1 å€‹æœˆè¶¨å‹¢
    K_FLAT  = 0.8     # ç›¤æ•´é–€æª»ï¼ˆ0.6~1.2ï¼›è¶Šå¤§è¶Šä¿å®ˆï¼‰

    os.makedirs("models", exist_ok=True)
    MODEL_PATH = f"models/{TICKER}_attn_lstm.keras"

    # ---------- Data ----------
    df = load_df_from_firestore(TICKER, collection=COLLECTION, days=500)
    df = ensure_latest_trading_row(df)
    df = add_features(df)

    # âœ… è¯æ±å°ˆå±¬ç‰¹å¾µï¼ˆå« OHLC + æ³¢å‹•/è·³ç©º/é‡èƒ½ï¼‰
    FEATURES = [
        "Close", "Open", "High", "Low",
        "Volume", "RSI", "MACD", "K", "D", "ATR_14",
        "HL_RANGE", "GAP", "VOL_REL",
        "TREND_60",
        "TREND_SLOPE_20"
    ]

    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(
            f"âš ï¸ Firestore è³‡æ–™ç¼ºæ¬„ä½ï¼š{missing}\n"
            f"è«‹ç¢ºèª catch_stock.py å¯«å› 8110.TW æ™‚åŒ…å« Open/High/Low/Close/Volumeï¼Œä¸”æŒ‡æ¨™æ¬„ä½å·²å¯«å…¥ã€‚"
        )

    if "RET_STD_20" not in df.columns:
        raise ValueError("âš ï¸ ç¼ºå°‘ RET_STD_20ï¼Œè«‹ç¢ºèª add_features() æœ‰è¢«å‘¼å«")

    df = df.dropna()

    # âœ… create_sequencesï¼šæœƒå›å‚³ y_trend3ï¼ˆ3é¡è¶¨å‹¢ï¼‰
    X, y_ret, y_dir, y_trend3, idx = create_sequences(
        df, FEATURES,
        steps=STEPS,
        window=LOOKBACK,
        trend_h=TREND_H,
        k_flat=K_FLAT
    )
    print(f"df rows: {len(df)} | X samples: {len(X)}")

    if len(X) < 60:
        raise ValueError("âš ï¸ å¯ç”¨åºåˆ—å¤ªå°‘ï¼ˆ<60ï¼‰ã€‚å»ºè­°ï¼šé™ä½ LOOKBACK æˆ–å¢åŠ  days/æª¢æŸ¥ NaNã€‚")

    # ---------- Time-series split ----------
    split = int(len(X) * 0.85)
    X_tr, X_va = X[:split], X[split:]
    y_ret_tr, y_ret_va = y_ret[:split], y_ret[split:]
    y_dir_tr, y_dir_va = y_dir[:split], y_dir[split:]
    y_tr3_tr, y_tr3_va = y_trend3[:split], y_trend3[split:]
    idx_tr, idx_va = idx[:split], idx[split:]

    # âœ… scaler.fit åƒ…ç”¨ train å€é–“ï¼ˆé¿å… leakageï¼‰
    train_end_date = pd.Timestamp(idx_tr[-1])
    df_for_scaler = df.loc[:train_end_date, FEATURES].copy()

    if len(df_for_scaler) < LOOKBACK + max(STEPS, TREND_H) + 5:
        raise ValueError("âš ï¸ train å€é–“å¤ªçŸ­ï¼Œç„¡æ³•ç©©å®š fit scalerã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´ LOOKBACKã€‚")

    sx = MinMaxScaler()
    sx.fit(df_for_scaler.values)

    def scale_X(Xb):
        n, t, f = Xb.shape
        return sx.transform(Xb.reshape(-1, f)).reshape(n, t, f)

    X_tr_s = scale_X(X_tr)
    X_va_s = scale_X(X_va)

    # ---------- Model ----------
    if os.path.exists(MODEL_PATH):
        print(f"âœ… è¼‰å…¥æ—¢æœ‰æ¨¡å‹ï¼š{MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    else:
        model = build_attention_lstm(
            (LOOKBACK, len(FEATURES)),
            STEPS,
            max_daily_normret=cfg["MAX_DAILY_NORMRET"],
            learning_rate=cfg["LR"],
            lstm_units=cfg["LSTM_UNITS"]
        )

    # âœ… çœŸæ­£æ™‚é–“åºåˆ— validationï¼ˆæœ€å¾Œ 15%ï¼‰
    model.fit(
        X_tr_s,
        {"return": y_ret_tr, "direction": y_dir_tr, "trend3": y_tr3_tr},
        validation_data=(X_va_s, {"return": y_ret_va, "direction": y_dir_va, "trend3": y_tr3_va}),
        epochs=120,
        batch_size=16,
        verbose=2,
        callbacks=[EarlyStopping(monitor="val_loss", patience=12, restore_best_weights=True)]
    )

    model.save(MODEL_PATH)
    print(f"ğŸ’¾ å·²å„²å­˜æ¨¡å‹ï¼š{MODEL_PATH}")

    # ---------- Predict (use validation tail as "latest unseen") ----------
    pred_ret, pred_dir, pred_tr3 = model.predict(X_va_s, verbose=0)

    raw_norm_returns = pred_ret[-1]         # 5æ—¥ normalized returnï¼ˆå·²é™å¹…ï¼‰
    p_dir = float(pred_dir[-1][0])          # 5æ—¥çœ‹æ¼²æ©Ÿç‡
    p_tr = pred_tr3[-1].astype(float)       # 20æ—¥è¶¨å‹¢ä¸‰é¡ [Down, Flat, Up]
    trend_label = ["Down", "Flat", "Up"][int(np.argmax(p_tr))]

    print(f"ğŸ“ˆ 5D çœ‹æ¼²æ©Ÿç‡: {p_dir:.2%}")
    print(f"ğŸ“Œ 20D è¶¨å‹¢: {trend_label} | P(Down/Flat/Up) = {p_tr[0]:.2f}/{p_tr[1]:.2f}/{p_tr[2]:.2f}")

    # ---------- Asof date ----------
    asof_date, is_today_trading = get_asof_trading_day(df)
    if not is_today_trading:
        print(f"â„¹ï¸ ä»Šæ—¥éäº¤æ˜“æ—¥ï¼Œ{TICKER} ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥ {asof_date.date()}")

    last_close = float(df.loc[asof_date, "Close"])

    # âœ… æŠŠ normalized return ä¹˜å›æ³¢å‹•å°ºåº¦ï¼ˆç”¨ asof çš„ RET_STD_20ï¼‰
    scale_last = float(df.loc[asof_date, "RET_STD_20"])
    if not np.isfinite(scale_last) or scale_last <= 0:
        scale_last = float(np.log(df["Close"].astype(float)).diff().rolling(20).std().iloc[-1])
    scale_last = max(scale_last, 1e-6)

    # ğŸ”§ Regime-based ampï¼ˆä¿ç•™çµ¦ 6M é€±æœŸéœ‡ç›ªç”¨ï¼‰
    trend60 = last_valid_value(df, "TREND_60", lookback=5)
    amp = 1.0
    if trend60 is not None:
        if trend60 > 1.0:
            amp = 1.4
        elif trend60 < -1.0:
            amp = 1.3
        elif abs(trend60) < 0.5:
            amp = 0.6

    print(f"ğŸ“Š Regime amp = {amp:.2f}")

    # ---------- 5D price projection ----------
    # âœ… æœ€æœ‰æ„Ÿä¿®æ­£ï¼š5æ—¥æ¨å›åƒ¹æ ¼ä¸è¦ä¹˜ ampï¼ˆé¿å…æ”¾å¤§å™ªéŸ³ï¼‰
    prices = []
    price = last_close
    for r_norm in raw_norm_returns:
        r = float(r_norm) * scale_last
        price *= np.exp(r)
        prices.append(price)

    seq = df.loc[:asof_date, "Close"].iloc[-10:].tolist()
    future = []
    for p in prices:
        seq.append(p)
        future.append({
            "Pred_Close": float(p),
            "Pred_MA5": float(np.mean(seq[-5:])),
            "Pred_MA10": float(np.mean(seq[-10:]))
        })

    future_df = pd.DataFrame(future)
    future_df["date"] = pd.bdate_range(
        start=asof_date + BDay(1),
        periods=STEPS
    )

    # âœ… é æ¸¬æ•¸å€¼è¼¸å‡º CSVï¼ˆæª”åå« tickerï¼‰
    os.makedirs("results", exist_ok=True)
    forecast_csv = f"results/{datetime.now():%Y-%m-%d}_{TICKER}_forecast.csv"
    future_df.to_csv(forecast_csv, index=False, encoding="utf-8-sig")

    # âœ… åœ–è¼¸å‡ºï¼ˆå…§å®¹ä¸å‹•ã€æª”åå« tickerï¼‰
    plot_and_save(df, future_df, ticker=TICKER)
    plot_backtest_error(df, ticker=TICKER)

    # ---------- 6M Outlook (advanced) ----------
    # ç”¨æœ€å¾Œä¸€ç­†æ–¹å‘æ©Ÿç‡åš confï¼ˆä½ åŸæœ¬è¨­è¨ˆï¼‰
    pred_dir_last = float(p_dir)

    plot_6m_trend_advanced(
        df=df,
        last_close=last_close,
        raw_norm_returns=raw_norm_returns,
        scale_last=scale_last,
        ticker=TICKER,
        asof_date=asof_date,
        amp=amp,
        pred_ret_all=pred_ret,          # âœ… å¯ç”¨ ensemble
        pred_dir_last=pred_dir_last,
        k_ens=20
    )
